import pandas as pd
import numpy as np
import re
import nltk

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from scipy.spatial.distance import cosine


class RecommenderContent:
    def __init__(self, df_content_path):
        # df = pd.read_csv('data/user-item-interactions.csv')
        self.df_content = pd.read_csv(df_content_path)
        del self.df_content['Unnamed: 0']

        self.words = None
        self.v = self.sigma = self.vt = None
        self.similarity_matrix = None
        self.preprocess()
        self.computeDistance()

    def preprocess(self):
        self.df_content['text'] = self.df_content['doc_description'] + " " + self.df_content['doc_full_name']
        self.df_content = self.df_content.dropna(subset='text')
        vect = CountVectorizer(tokenizer=self.tokenize)
        X = vect.fit_transform(self.df_content['text'])
        transformer = TfidfTransformer()
        X = transformer.fit_transform(X)
        self.u, self.sigma, self.vt = np.linalg.svd(X.toarray())
        self.words = vect.get_feature_names_out()

    def tokenize(self, text):
        """
        This function tokenize plain text and output the corresponding tokens

        Input:
        text - (str) the input string

        OUTPUT:
        tokens - (list) a list tokens

        """
        # normalize case and remove punctuation

        text = re.sub(r'[^a-zA-Z]', ' ', text.lower())
        # tokenize text
        tokens = word_tokenize(text)
        lemmatizer = WordNetLemmatizer()
        #  lemmatize and remove stop words
        tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stopwords.words("english")]
        return tokens

    def computeDistance(self, top_feature=654):
        """
        This function compute the eculedian distance between each article and words in the hidden feature space

        INPUT:
        top_feature - (int) the number of hidden feature used for computation
        u - (numpy.ndarray) the U matrix generated by SVD
        vt - (numpy.ndarray) the VT matrix generated by SVD
        """

        u_temp = self.u[:, :top_feature]
        vt_temp = self.vt[:top_feature, :]
        m = self.u.shape[0]
        n = self.vt.shape[1]
        self.similarity_matrix = np.zeros((m, n))
        for i in range(m):
            for j in range(n):
                u_vec = u_temp[i, :]
                vt_vec = vt_temp[:, j]
                self.similarity_matrix[i, j] = -np.linalg.norm(u_vec - vt_vec)

    def getArticleTitle(self, article_id):
        return list(self.df_content.loc[self.df_content['article_id'].isin(article_id), 'doc_full_name'])

    def make_content_recs(self, term, top_n=5):
        """
        This function can output terms related to a input article_id or recommend relevant article given a input term.

        INPUT:
        term - (str/int) This can be an input article id or term.
        id_style -(str) This is an indicator of the kind of term. 'article_id' m
        top_n -(int) An integer indicating the max number of recommendation the function would return
        df_content - (pandas dataframe)
        words - (list) a list of tokens.
        similarity_matrix - (np.ndarray) the similarity matrix between articles and words

        OUTPUT:
        similar_words/similarity_article - (list) a list of words similar to the input article or a list
                                                  of articles similar to the input terms
        """
        try:
            flag = self.df_content['text'].str.contains(term)
            target_articleId = self.df_content.loc[flag, 'article_id']
            row_index = [np.where(self.df_content['article_id'] == articleId)[0][0] for articleId in target_articleId]
            col_index = np.where(np.array(self.words) == term)[0][0]
            if row_index is not None and col_index is not None:
                similarity_article = list(
                    sorted(zip(target_articleId, self.similarity_matrix[row_index, col_index]), key=lambda x: x[1],
                           reverse=True))
                similarity_article = [a for a, s in similarity_article][:top_n]
                return self.getArticleTitle(similarity_article)
            elif row_index is None and col_index is not None:
                target_articleId = self.df_content['article_id']
                similarity_article = list(
                    sorted(zip(target_articleId, self.similarity_matrix[:, col_index]), key=lambda x: x[1],
                           reverse=True))
                similarity_article = [a for a, s in similarity_article][:top_n]
                return self.getArticleTitle(similarity_article)
        except:
            print('The input term does not exist in the database.')
            return ['The input term does not exist in the database.']
