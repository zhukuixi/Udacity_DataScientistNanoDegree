{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 7716044,
          "sourceType": "datasetVersion",
          "datasetId": 4506348
        },
        {
          "sourceId": 7723890,
          "sourceType": "datasetVersion",
          "datasetId": 4512191
        }
      ],
      "dockerImageVersionId": 30458,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/v2/resize:fit:828/format:webp/1*vyXO4v0OQsmcctUcx6jL3w.png?)"
      ],
      "metadata": {
        "id": "Du_nLF4uSI_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "- [1. Introduction](#Introduction)\n",
        "    - [1.1 Business Understanding](#business-understanding)\n",
        "- [2. Exploratory Data Analysis and Data Modeling](#eda)\n",
        "    - [2.1 Data Understanding](#data-understanding)        \n",
        "    - [2.2 Data Preparation](#data-preparation)  \n",
        "        - [2.2.1 Missing Value Insepction](#missing-value)\n",
        "        - [2.2.2 Data Wrangling](#data-wrangling)\n",
        "        - [2.2.3 Exploratory Data Analysis](#eda)  \n",
        "        - [2.2.4 Feature Selection](#fs)\n",
        "    - [2.3 Data Modeling](#dm)\n",
        "    \n",
        "- [3. Conclusions](#conclusion)\n"
      ],
      "metadata": {
        "id": "3wBVdL6BSI_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "In this notebook I will analyze the [Sparkify Dataset](https://www.kaggle.com/datasets/kuixizhu/sparkify-small-log) from Udacity Data Scientist Nano Degree Program. It will follow and CRoss Industry Standard Process for Data Mining (CRISP-DM) which is consisted of the following steps:\n",
        "1. Business Understanding\n",
        "2. Data Understanding\n",
        "3. Data Preparation\n",
        "4. Data Modeling\n",
        "5. Result Evaluation and Conclusions"
      ],
      "metadata": {
        "id": "O47iHoI-SI_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.1 Business Understanding\n",
        "<a id=\"business-understanding\"></a>"
      ],
      "metadata": {
        "id": "uBBWFdgGpMEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sparkify is a fictitious music streaming platform with a vast user base engaging with the service daily. Similar to leading platforms such as Spotify and Pandora, users have the option to utilize a free account, which features intermittent advertisements between songs, or to upgrade to a premium account offering ad-free listening and enhanced sound quality for a monthly fee. Given that revenue from premium accounts significantly contributes to the company's overall income, our objective is to develop a classifier capable of identifying potential churn customers, thereby safeguarding millions in revenue for the business."
      ],
      "metadata": {
        "id": "TE5zzkpGpMEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<a id=\"data-understanding\"></a>\n",
        "# 2.1 Data Understanding\n"
      ],
      "metadata": {
        "id": "vT4VKOBbpMEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from pyspark.sql.types import DoubleType\n",
        "import math\n",
        "from pyspark.sql.functions import *\n",
        "from scipy import stats\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "from pyspark.ml.feature import StandardScaler,MinMaxScaler\n",
        "from pyspark.ml.classification import LogisticRegression,RandomForestClassifier,GBTClassifier,LinearSVC\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.ml.feature import PCA\n",
        "\n",
        "\n",
        "pd.options.display.max_columns = None\n",
        "pd.options.display.max_rows = None"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "KNwm3Df4pMEl",
        "_kg_hide-input": false,
        "execution": {
          "iopub.status.busy": "2024-03-02T16:28:24.050282Z",
          "iopub.execute_input": "2024-03-02T16:28:24.050585Z",
          "iopub.status.idle": "2024-03-02T16:29:02.270741Z",
          "shell.execute_reply.started": "2024-03-02T16:28:24.050559Z",
          "shell.execute_reply": "2024-03-02T16:29:02.269821Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "## Create spark session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Sparkify\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "## Read in small sparkify dataset for EDA\n",
        "#event_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
        "event_data = \"/kaggle/input/sparkify-small-log/mini_sparkify_event_data.json\"\n",
        "user_log_small = spark.read.json(event_data)\n",
        "user_log_small.printSchema()"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "_kg_hide-input": false,
        "id": "9P3Rm2NYpMEr",
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:02.272223Z",
          "iopub.execute_input": "2024-03-02T16:29:02.272488Z",
          "iopub.status.idle": "2024-03-02T16:29:18.277268Z",
          "shell.execute_reply.started": "2024-03-02T16:29:02.272458Z",
          "shell.execute_reply": "2024-03-02T16:29:18.275493Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there is no document explaining the meaning of each feature, I will take a look at the data and get a sense of the data."
      ],
      "metadata": {
        "id": "7ul8IlKqSI_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_log_small.toPandas().head(3)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:18.279894Z",
          "iopub.execute_input": "2024-03-02T16:29:18.280800Z",
          "iopub.status.idle": "2024-03-02T16:29:25.859152Z",
          "shell.execute_reply.started": "2024-03-02T16:29:18.280749Z",
          "shell.execute_reply": "2024-03-02T16:29:25.858229Z"
        },
        "trusted": true,
        "id": "j9cC0PvHSI_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the features, they can be divided into 3 categories: user features, song features and user-platform interaction features.\n"
      ],
      "metadata": {
        "id": "kgSdhbtxSI_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**User Feature**\n",
        "\n",
        "| Feature    | DataType | Description |\n",
        "| --- | --- | ---|\n",
        "|userId|string|user identifier|\n",
        "|firstName|string|user’s first name|\n",
        "|lastName|string|user’s last name|\n",
        "|gender|string(categorical)|user’s gender (M and F)|\n",
        "|registration|long|user’s registration timestamp|\n",
        "|level|string(categorical)|subscription level (free and paid)|\n",
        "|location|string|user’s location|\n",
        "\n",
        "\n",
        "**Song Feature**\n",
        "\n",
        "| Feature    | DataType | Description |\n",
        "| --- | --- | ---|\n",
        "|song|string|song name|\n",
        "|artist|string|artist name|\n",
        "|length|double|song’s length in seconds|\n",
        "\n",
        "**User-platform Interaction Feature**\n",
        "\n",
        "| Feature    | DataType | Description |\n",
        "| --- | ---| ---|\n",
        "|auth|string(categorical)|authentication level (Logged In, Logged Out, Cancelled, Guest)|\n",
        "|itemInSession|long|log count in a given session|\n",
        "|method|string(categorical)|http request method (GET and PUT)|\n",
        "|page|string(categorical)|type of interaction (NextSong, Home, **Cancellation Confirmation**, etc.)|\n",
        "|sessionId|long|session to which the log belongs to|\n",
        "|status|long(categorical)|http status code (200, 307 and 404)|\n",
        "|ts|long|timestamp of a given log|\n",
        "|userAgent|string|agent used by the user to access the streaming service|\n",
        "\n",
        "\n",
        "In the page varaible, **Cancellation Confirmation** is an indicator of customer churn."
      ],
      "metadata": {
        "id": "j4aVkGq2pMEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"data-preparation\"></a>\n",
        "# 2.2 Data Preparation\n"
      ],
      "metadata": {
        "id": "uWR0_Q2gpMEs",
        "_kg_hide-input": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we conduct any exploration data analysis (EDA), we need to do some data wrangling to proprocess the data."
      ],
      "metadata": {
        "id": "in9ismfcSI_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"missing-value\"></a>\n",
        "# 2.2.1 Missing Value Insepection"
      ],
      "metadata": {
        "id": "MviP2bWJSI_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import isnan, when, count, col,round,asc,desc\n",
        "nrow = user_log_small.count()\n",
        "user_log_small.select([round(100*count(when(col(c).isNull(), c))/nrow,2).alias(c) for c in user_log_small.columns]).show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:25.861943Z",
          "iopub.execute_input": "2024-03-02T16:29:25.862623Z",
          "iopub.status.idle": "2024-03-02T16:29:28.632743Z",
          "shell.execute_reply.started": "2024-03-02T16:29:25.862593Z",
          "shell.execute_reply": "2024-03-02T16:29:28.631814Z"
        },
        "trusted": true,
        "id": "5p_8aRneSI_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "There are two distinct categories of missing values within the dataset, each pertaining to different aspects of the data:\n",
        "\n",
        "Song-Related Features: This group comprises attributes directly associated with songs, such as artist, song length, and song title. The proportion of missing values within this category amounts to 20.38% of the entire dataset.\n",
        "\n",
        "User-Related Features: In contrast, this group encompasses attributes relevant to user profiles, including first name, gender, last name, location, registration, and user agent. The percentage of missing values within this set of features is notably lower, accounting for 2.91% of the dataset."
      ],
      "metadata": {
        "id": "1uc4aMtBSI_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_log_small = user_log_small.withColumn('song_na',col('song').isNull())\n",
        "user_log_small = user_log_small.withColumn('user_na',col('firstName').isNull())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:28.633685Z",
          "iopub.execute_input": "2024-03-02T16:29:28.634006Z",
          "iopub.status.idle": "2024-03-02T16:29:28.674812Z",
          "shell.execute_reply.started": "2024-03-02T16:29:28.633974Z",
          "shell.execute_reply": "2024-03-02T16:29:28.673814Z"
        },
        "trusted": true,
        "id": "RhjnR_SWSI_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_log_small.groupBy('song_na','page').count().orderBy(desc('count')).show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:28.675566Z",
          "iopub.execute_input": "2024-03-02T16:29:28.675840Z",
          "iopub.status.idle": "2024-03-02T16:29:30.479858Z",
          "shell.execute_reply.started": "2024-03-02T16:29:28.675803Z",
          "shell.execute_reply": "2024-03-02T16:29:30.478947Z"
        },
        "trusted": true,
        "id": "iWC0VdKlSI_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_log_small.groupBy('user_na','auth').count().orderBy(desc('count')).show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:30.480756Z",
          "iopub.execute_input": "2024-03-02T16:29:30.481042Z",
          "iopub.status.idle": "2024-03-02T16:29:31.297029Z",
          "shell.execute_reply.started": "2024-03-02T16:29:30.481012Z",
          "shell.execute_reply": "2024-03-02T16:29:31.296099Z"
        },
        "trusted": true,
        "id": "9SJj8pEoSI_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After comparing the distribution of page and auth in samples with and without missing values, we found that:\n",
        "1. Song related features are missing in samples where the page equals to **NextSong**.\n",
        "2. Users realted features are missing in samples where the auth values are **Logged Out** and **Guest**.\n",
        "\n",
        "Both of these observation makes perfect sense. As we are going to build a classifier on user level, we only need to remove samples containing NULL value in user related features."
      ],
      "metadata": {
        "id": "-alo6US7SI_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_log_small = user_log_small.na.drop(subset=['firstName'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:31.297978Z",
          "iopub.execute_input": "2024-03-02T16:29:31.298316Z",
          "iopub.status.idle": "2024-03-02T16:29:31.319040Z",
          "shell.execute_reply.started": "2024-03-02T16:29:31.298283Z",
          "shell.execute_reply": "2024-03-02T16:29:31.318087Z"
        },
        "trusted": true,
        "id": "aJwV0SrtSI_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"data-wrangling\"></a>\n",
        "# 2.2.2 Data Wrangling"
      ],
      "metadata": {
        "id": "CxwZ09iLSI_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Churn User\n",
        "Before we conduct feature engineering and EDA, we need to define the churn user.\n",
        "Here, for each user,  I would definite the user as churn user if **Cancellation Confirmation** ever appear in the user's **page** column."
      ],
      "metadata": {
        "id": "mTPuMS4pSI_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate churn feature\n",
        "user_log_small.createOrReplaceTempView('user_log_small_table')\n",
        "\n",
        "user_churn = spark.sql('''\n",
        "        SELECT userId,\n",
        "               MAX(IF(page = \"Cancellation Confirmation\",1,0)) as churn\n",
        "        FROM user_log_small_table\n",
        "        GROUP BY userId\n",
        "''')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:31.320039Z",
          "iopub.execute_input": "2024-03-02T16:29:31.320317Z",
          "iopub.status.idle": "2024-03-02T16:29:31.492033Z",
          "shell.execute_reply.started": "2024-03-02T16:29:31.320286Z",
          "shell.execute_reply": "2024-03-02T16:29:31.490863Z"
        },
        "trusted": true,
        "id": "dduKh2KOSI_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering\n",
        "\n",
        "\n",
        "\n",
        "Let's start with the user related features.\n",
        "\n",
        "### User Feature\n",
        "\n",
        "| Feature    | DataType | Description |\n",
        "| --- | --- | ---|\n",
        "|userId|string|user identifier|\n",
        "|firstName|string|user’s first name|\n",
        "|lastName|string|user’s last name|\n",
        "|gender|string(categorical)|user’s gender (M and F)|\n",
        "|registration|long|user’s registration timestamp|\n",
        "|level|string(categorical)|subscription level (free and paid)|\n",
        "|location|string|user’s location|\n",
        "\n",
        "\n",
        "I choose gender, level, location and registration as they are informative features.  \n",
        "\n",
        "**gender**  \n",
        "- ***gender_cate***: Binary encoding 'M' and 'F' into 1 and 0 for gender representation.\n",
        "\n",
        "\n",
        "**registraiton**  \n",
        "- ******registration_duration_day***: The length of time since user's registration (unit: days).\n",
        "\n",
        "**level**\n",
        "\n",
        "For the \"level\" feature, it presents an intriguing insight into user behavior, particularly considering the dynamic nature where users can freely switch between free and paid statuses at any point. Specifically, the following aspects are of interest for each user:\n",
        "\n",
        "\n",
        "1. **Frequency of Transition**: This denotes the number of times a user switches from free to paid status, reflecting their decision-making patterns.\n",
        "\n",
        "2. **Reversion Frequency**: Indicates the number of times a user reverts from paid to free status, shedding light on potential dissatisfaction or preference changes.\n",
        "\n",
        "3. **Average Duration in Free Status**: This metric calculates the average duration a user's account remains in the free level, offering insights into user engagement and retention in the free tier.\n",
        "\n",
        "4. **Average Duration in Paid Status**: Similarly, this measure computes the average duration a user's account stays in the paid level, revealing patterns of user commitment and investment in premium services.\n",
        "\n",
        "5. **The User's Latest Account Level**: It is the user's latest account status (free/paid).\n",
        "\n",
        "6. **Time Since Registration to First Interaction**: This duration reflects the engagement level of users with the platform, capturing their immediacy in utilizing platform features post-registration.\n",
        "\n",
        "Correspondingly, I will generate these feaures:\n",
        "- ***free_to_paid***: The number of transitions from free to paid level\n",
        "- ***paid_to_free***: The number of transitions from paid to free level\n",
        "- ***level_free_avg_duration_day***: The average duration of time the user account remains in the free level (unit: days)\n",
        "- ***level_paid_avg_duration_day***: The average duration of time the user account remains in the paid level (unit: days)\n",
        "- ***last_level***:  The user's lastest account level.last_level - The user's lastest account level.\n",
        "- ***duration_beforeFirstAction***: The  Duration of time between user registration and the first platform interaction (unit: days)\n",
        "\n",
        "**location**\n",
        "\n",
        "Here I will extract the state name and map it to [state-level average household income in the United States](https://www2.census.gov/programs-surveys/cps/tables/time-series/historical-income-households/h08.xls). This new feature would provide insights into regional socioeconomic status.\n",
        "\n",
        "- ***location_income***: The average house-hold income of the location\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CvV6_5W8SI_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate features related to user gender:\n",
        "# Feature 1: gender_cate - Mapping gender values: 'M' to 1, 'F' to 0.\n",
        "\n",
        "\n",
        "user_gender = spark.sql(\n",
        "    '''\n",
        "    SELECT userId,\n",
        "           SUBSTRING_INDEX(ANY_VALUE(location),', ',-1) as location,\n",
        "           ANY_VALUE(gender) as gender,\n",
        "           IF(ANY_VALUE(gender)='M',1,0) as gender_code\n",
        "    FROM user_log_small_table\n",
        "    GROUP BY userId\n",
        "    '''\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:31.499592Z",
          "iopub.execute_input": "2024-03-02T16:29:31.501011Z",
          "iopub.status.idle": "2024-03-02T16:29:31.564157Z",
          "shell.execute_reply.started": "2024-03-02T16:29:31.500976Z",
          "shell.execute_reply": "2024-03-02T16:29:31.563246Z"
        },
        "trusted": true,
        "id": "r8xZd3fhSI_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate features related to user registration:\n",
        "# Feature 1: registration_duration_day - The length of time since user's registration (unit: days).\n",
        "\n",
        "user_duration = spark.sql('''\n",
        "    SELECT  userID,\n",
        "            ROUND((MAX(ts) - MAX(registration))/(1000*3600*24),2) as registration_duration_day\n",
        "    FROM user_log_small_table\n",
        "    GROUP BY userId\n",
        "''')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:31.565098Z",
          "iopub.execute_input": "2024-03-02T16:29:31.565420Z",
          "iopub.status.idle": "2024-03-02T16:29:31.607228Z",
          "shell.execute_reply.started": "2024-03-02T16:29:31.565385Z",
          "shell.execute_reply": "2024-03-02T16:29:31.606315Z"
        },
        "trusted": true,
        "id": "t_IS_IAVSI_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate features related to user account level transitions and durations:\n",
        "# Feature 1: free_to_paid - Number of transitions from free to paid level\n",
        "# Feature 2: paid_to_free - Number of transitions from paid to free level\n",
        "# Feature 3: level_free_avg_duration_day - Average duration of time the user account remains in the free level (unit: days)\n",
        "# Feature 4: level_paid_avg_duration_day - Average duration of time the user account remains in the paid level (unit: days)\n",
        "# Feature 5: last_level - The user's lastest account level.\n",
        "# Feature 6: duration_beforeFirstAction - Duration of time between user registration and the first platform interaction (unit: days)\n",
        "\n",
        "user_level_switch = spark.sql('''\n",
        "    WITH CTE AS(\n",
        "        SELECT userId,\n",
        "               IF(level='paid' and LAG(level) over w='free',1,0) AS free_to_paid,\n",
        "               IF(level='free' and LAG(level) over w='paid',1,0) AS paid_to_free ,\n",
        "               IF(LAST(level) over w='paid',1,0) as last_level\n",
        "        FROM user_log_small_table\n",
        "        WINDOW w as (PARTITION BY userId ORDER BY ts)\n",
        "    )\n",
        "\n",
        "    SELECT userId,\n",
        "           SUM(free_to_paid) as level_free_to_paid,\n",
        "           SUM(paid_to_free) as level_paid_to_free,\n",
        "           ANY_VALUE(last_level) as last_level\n",
        "    FROM CTE\n",
        "    GROUP BY userId\n",
        "''')\n",
        "\n",
        "\n",
        "user_level_duration = spark.sql('''\n",
        "    WITH CTE AS(\n",
        "        SELECT userId,\n",
        "               IF(level != LAG(level) over w,1,0) AS change_flag,\n",
        "               level,\n",
        "               ts,\n",
        "               registration\n",
        "        FROM user_log_small_table\n",
        "        WINDOW w as (PARTITION BY userId ORDER BY ts)\n",
        "    ),\n",
        "\n",
        "    CTE_phase AS(\n",
        "        SELECT userId,\n",
        "               SUM(change_flag) over w as phase,\n",
        "               level,\n",
        "               ts,\n",
        "               registration\n",
        "        FROM CTE\n",
        "        WINDOW w as (PARTITION BY userId ORDER BY ts ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\n",
        "    ),\n",
        "\n",
        "    CTE_phase_duration AS(\n",
        "        SELECT userId,\n",
        "               level,\n",
        "               CASE WHEN phase > 0 THEN ROUND((MAX(ts) - MIN(ts))/(1000*3600*24),2)\n",
        "                    WHEN phase = 0 THEN ROUND((MAX(ts) - ANY_VALUE(registration))/(1000*3600*24),2)\n",
        "                    END as duration_day ,\n",
        "               IF(phase = 0,ROUND((MIN(ts) - ANY_VALUE(registration))/(1000*3600*24),2),0) as duration_beforeFirstAction\n",
        "        FROM CTE_phase\n",
        "        GROUP BY userId,level,phase\n",
        "    ),\n",
        "\n",
        "    RESULT AS(\n",
        "        SELECT userId,\n",
        "               level,\n",
        "               COUNT(*) as phase_cnt,\n",
        "               ROUND(AVG(duration_day),2) as level_avg_duration_day ,\n",
        "               SUM(duration_beforeFirstAction) as duration_beforeFirstAction\n",
        "        FROM CTE_phase_duration\n",
        "        GROUP BY userId,level\n",
        "    )\n",
        "\n",
        "    SELECT userId,\n",
        "           MAX(IF(level=\"free\",phase_cnt,0)) as free_cnt,\n",
        "           MAX(IF(level=\"paid\",phase_cnt,0)) as paid_cnt,\n",
        "           MAX(IF(level=\"free\",level_avg_duration_day,0)) as level_free_avg_duration_day,\n",
        "           MAX(IF(level=\"paid\",level_avg_duration_day,0)) as level_paid_avg_duration_day,\n",
        "           MAX(duration_beforeFirstAction) as duration_beforeFirstAction\n",
        "    FROM RESULT\n",
        "    GROUP BY userId\n",
        "\n",
        "''')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:31.609741Z",
          "iopub.execute_input": "2024-03-02T16:29:31.610149Z",
          "iopub.status.idle": "2024-03-02T16:29:31.921293Z",
          "shell.execute_reply.started": "2024-03-02T16:29:31.610107Z",
          "shell.execute_reply": "2024-03-02T16:29:31.920412Z"
        },
        "trusted": true,
        "id": "cXjgmh-HSI_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Genreate feature for location\n",
        "# Feature 1: location_income - The average house-hold income of the location\n",
        "\n",
        "state= pd.read_csv(\"/kaggle/input/states-average-household-income-2018/states.csv\")\n",
        "spark_state = spark.createDataFrame(state)\n",
        "#spark_state = spark.read.csv(\"s3://spark-udacity-joo/states.csv\", header=True)\n",
        "spark_state.createOrReplaceTempView('state_table')\n",
        "getLocation = udf(lambda x:x.split(\", \")[-1])\n",
        "\n",
        "user_log_small = user_log_small.withColumn('location',getLocation(user_log_small.location))\n",
        "user_log_small.createOrReplaceTempView('user_log_small_table')\n",
        "\n",
        "\n",
        "location_income = spark.sql('''\n",
        "    WITH log_state AS(\n",
        "        SELECT DISTINCT SUBSTRING_INDEX(LOCATION,', ',-1) as state_abb\n",
        "        FROM user_log_small_table\n",
        "    ),\n",
        "\n",
        "    State_Income AS (\n",
        "        SELECT Abbreviation,\n",
        "               DOUBLE(REPLACE(Income,\",\",\"\")) as Income\n",
        "        FROM  state_table\n",
        "    )\n",
        "\n",
        "    SELECT state_abb as location,\n",
        "           AVG(Income) location_income\n",
        "    FROM log_state\n",
        "         JOIN State_Income ON state_abb = Abbreviation\n",
        "                           OR SUBSTRING_INDEX(SUBSTRING_INDEX(state_abb,'-',1),'-',-1) = Abbreviation\n",
        "                           OR SUBSTRING_INDEX(SUBSTRING_INDEX(state_abb,'-',2),'-',-1) = Abbreviation\n",
        "                           OR SUBSTRING_INDEX(SUBSTRING_INDEX(state_abb,'-',3),'-',-1) = Abbreviation\n",
        "                           OR SUBSTRING_INDEX(SUBSTRING_INDEX(state_abb,'-',4),'-',-1) = Abbreviation\n",
        "    GROUP BY state_abb\n",
        "    '''\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:31.922320Z",
          "iopub.execute_input": "2024-03-02T16:29:31.922608Z",
          "iopub.status.idle": "2024-03-02T16:29:32.185485Z",
          "shell.execute_reply.started": "2024-03-02T16:29:31.922577Z",
          "shell.execute_reply": "2024-03-02T16:29:32.184436Z"
        },
        "trusted": true,
        "id": "_qjflF8kSI_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put the new features together and normalize the time-duration realted features."
      ],
      "metadata": {
        "id": "DFYPMAT3SI_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_user = user_churn.join(user_gender,on='userId') \\\n",
        "                  .join(user_duration,on='userId') \\\n",
        "                  .join(user_level_switch,on='userId') \\\n",
        "                  .join(user_level_duration,on='userId') \\\n",
        "                  .join(location_income,on='location')\n",
        "\n",
        "data_user = data_user.withColumn('level_free_avg_duration_ratio',(data_user.level_free_avg_duration_day) /  (data_user.registration_duration_day))\n",
        "data_user = data_user.withColumn('level_paid_avg_duration_ratio',data_user.level_paid_avg_duration_day /  data_user.registration_duration_day)\n",
        "data_user = data_user.withColumn('level_free_duration_ratio',data_user.level_free_avg_duration_day * data_user.free_cnt /  data_user.registration_duration_day)\n",
        "data_user = data_user.withColumn('level_paid_duration_ratio',data_user.level_paid_avg_duration_day * data_user.paid_cnt /  data_user.registration_duration_day)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:32.186573Z",
          "iopub.execute_input": "2024-03-02T16:29:32.186936Z",
          "iopub.status.idle": "2024-03-02T16:29:32.822615Z",
          "shell.execute_reply.started": "2024-03-02T16:29:32.186901Z",
          "shell.execute_reply": "2024-03-02T16:29:32.821644Z"
        },
        "trusted": true,
        "id": "C5UoR8IoSI_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Song Feature\n",
        "\n",
        "| Feature    | DataType | Description |\n",
        "| --- | --- | ---|\n",
        "|song|string|song name|\n",
        "|artist|string|artist name|\n",
        "|length|double|song’s length in seconds|"
      ],
      "metadata": {
        "id": "MqkPBXFpSI_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, I aim to understand users' behavior patterns, particularly focusing on their song listening habits, including the variety of songs they listen to, whether they listen to full tracks or just parts, and their interaction patterns with the platform. By examining user actions throughout their time on the platform, I seek to uncover insights such as whether users engage with the platform immediately after registration or if they maintain regular interaction over time."
      ],
      "metadata": {
        "id": "5zHhnY3BSI_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I will create these features:  \n",
        "- ***song_cnt***: The number of song listened by user\n",
        "- ***distinct_song_cnt***: The number of unique song listened by user\n",
        "- ***song_entropy***: The entropy of the probability distribution of played songs by user\n",
        "- ***artist_entropy***:  The entropy of the probability distribution of the artists of played songs by user\n",
        "- ***song_percent_avg***: The mean value of percentage of a song listened by user.\n",
        "- ***song_percent_std***: The standard deviation value of percentage of a song listened by user.\n",
        "\n",
        "- ***action_ts_avg***: The average timing of user actions.\n",
        "- ***action_ts_std***: The variability in the timing of user actions.\n",
        "                          \n",
        "- ***action_hist_0 ~ action_hist_9*** : Represents the distribution of timestamp of user actions.                                            Each feature (action_hist_i) corresponds to a bin in the histogram of normalized timestamps.\n",
        "The values represent the frequency of user actions falling into each bin."
      ],
      "metadata": {
        "id": "Ta24jbtwSI_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_log_small.createOrReplaceTempView('user_log_small_table')\n",
        "\n",
        "# Genreate feature for song\n",
        "# Feature 1: song_cnt - The number of song listened by user\n",
        "# Feature 2: distinct_song_cnt - The number of unique song listened by user\n",
        "\n",
        "song_cnt = spark.sql('''\n",
        "    SELECT userId,\n",
        "           COUNT(song) as song_cnt,\n",
        "           COUNT(DISTINCT song) as distinct_song_cnt\n",
        "    FROM user_log_small_table\n",
        "    WHERE song IS NOT NULL\n",
        "    GROUP BY userId\n",
        "\n",
        "''')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:32.823634Z",
          "iopub.execute_input": "2024-03-02T16:29:32.823947Z",
          "iopub.status.idle": "2024-03-02T16:29:32.851066Z",
          "shell.execute_reply.started": "2024-03-02T16:29:32.823914Z",
          "shell.execute_reply": "2024-03-02T16:29:32.850068Z"
        },
        "trusted": true,
        "id": "6MaH1sTsSI_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Genreate feature for song\n",
        "# Feature 1: song_entropy - The entropy of the probability distribution of played songs by user\n",
        "# Feature 2: artist_entropy - The entropy of the probability distribution of the artists of played songs by user\n",
        "\n",
        "def entropy(probabilities):\n",
        "  \"\"\"Computes entropy of a list of probabilities.\n",
        "\n",
        "  Args:\n",
        "    probabilities: A list of probabilities.\n",
        "\n",
        "  Returns:\n",
        "    The entropy of the list of probabilities.\n",
        "  \"\"\"\n",
        "\n",
        "  entropy = 0.0\n",
        "\n",
        "  for probability in probabilities:\n",
        "    entropy -= probability * math.log2(probability)\n",
        "  return entropy\n",
        "\n",
        "\n",
        "entropy_udf = udf(entropy, DoubleType())\n",
        "\n",
        "\n",
        "song_proba = spark.sql('''\n",
        "    WITH CTE AS(\n",
        "        SELECT userId,\n",
        "               song,\n",
        "               COUNT(*) as song_cnt\n",
        "        FROM user_log_small_table\n",
        "        WHERE song IS NOT NULL\n",
        "        GROUP BY userId,song\n",
        "    ),\n",
        "\n",
        "    total_table AS(\n",
        "        SELECT userId,\n",
        "               COUNT(*) as total_song_cnt\n",
        "        FROM user_log_small_table\n",
        "        WHERE song IS NOT NULL\n",
        "        GROUP BY userId\n",
        "    )\n",
        "\n",
        "    SELECT userId,\n",
        "           song,\n",
        "           song_cnt / total_song_cnt as proba\n",
        "    FROM CTE\n",
        "         JOIN total_table USING(userId)\n",
        "\n",
        "''')\n",
        "\n",
        "artist_proba = spark.sql('''\n",
        "    WITH CTE AS(\n",
        "        SELECT userId,\n",
        "               artist,\n",
        "               COUNT(*) as artist_cnt\n",
        "        FROM user_log_small_table\n",
        "        WHERE artist IS NOT NULL\n",
        "        GROUP BY userId,artist\n",
        "    ),\n",
        "\n",
        "    total_table AS(\n",
        "        SELECT userId,\n",
        "               COUNT(*) as total_artist_cnt\n",
        "        FROM user_log_small_table\n",
        "        WHERE artist IS NOT NULL\n",
        "        GROUP BY userId\n",
        "    )\n",
        "\n",
        "    SELECT userId,\n",
        "           artist,\n",
        "           artist_cnt / total_artist_cnt as proba\n",
        "    FROM CTE\n",
        "         JOIN total_table USING(userId)\n",
        "''')\n",
        "\n",
        "\n",
        "\n",
        "song_entropy = song_proba.groupBy('userId').agg(entropy_udf(collect_list('proba')).alias('song_entropy'))\n",
        "artist_entropy = artist_proba.groupBy('userId').agg(entropy_udf(collect_list('proba')).alias('artist_entropy'))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:32.852114Z",
          "iopub.execute_input": "2024-03-02T16:29:32.852422Z",
          "iopub.status.idle": "2024-03-02T16:29:32.990890Z",
          "shell.execute_reply.started": "2024-03-02T16:29:32.852388Z",
          "shell.execute_reply": "2024-03-02T16:29:32.989909Z"
        },
        "trusted": true,
        "id": "OPNoYD0JSI_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Genreate feature for song listening behavior\n",
        "# Feature 1: song_percent_avg - The mean value of percentage of a song listened by user.\n",
        "# Feature 2: song_percent_std - The standard deviation value of percentage of a song listened by user.\n",
        "\n",
        "song_percent = spark.sql('''\n",
        "        WITH CTE AS(\n",
        "            SELECT userId,\n",
        "                   CASE WHEN ROUND(100*((LEAD(ts) over w ) - ts) /(1000*length),2) > 100 THEN 100\n",
        "                        ELSE ROUND(100*((LEAD(ts) over w ) - ts) /(1000*length),2)\n",
        "                   END as song_percentage\n",
        "            FROM user_log_small_table\n",
        "            WINDOW w as (PARTITION BY userId, sessionId ORDER BY ts)\n",
        "        )\n",
        "\n",
        "        SELECT userId,\n",
        "               AVG(song_percentage) as song_percent_avg,\n",
        "               STD(song_percentage) as song_percent_std\n",
        "        FROM CTE\n",
        "        GROUP BY userId\n",
        "    '''\n",
        ")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:32.993435Z",
          "iopub.execute_input": "2024-03-02T16:29:32.993792Z",
          "iopub.status.idle": "2024-03-02T16:29:33.067164Z",
          "shell.execute_reply.started": "2024-03-02T16:29:32.993737Z",
          "shell.execute_reply": "2024-03-02T16:29:33.066269Z"
        },
        "trusted": true,
        "id": "gueyYa5bSI_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Genreate feature for the timestamp of user's action\n",
        "# Feature 1: action_ts_avg - Represents the average timing of user actions,\n",
        "#                            It is computed as the mean value of the user action's normalized timestamps.\n",
        "# Feature 2: action_ts_std - Represents the variability in the timing of user actions.\n",
        "#                            It is computed as the standard deviation of the user action's normalized timestamps.\n",
        "# Feature 3: action_hist_0 to action_hist_9 - Represents the distribution of timestamp of user actions.\n",
        "#                                            Each feature (action_hist_i) corresponds to a bin in the histogram of normalized timestamps.\n",
        "#                                            The values represent the frequency of user actions falling into each bin.\n",
        "#All metrics are based on the normalized value between 0 and 1 according to the lifespan of the user account.\n",
        "\n",
        "\n",
        "\n",
        "action_ts = spark.sql('''\n",
        "        SELECT userId,\n",
        "               (ts -  (MIN(registration) over w)) / ((MAX(ts) over w) - (MIN(registration) over w)) as ts_percent\n",
        "        FROM user_log_small_table\n",
        "        WINDOW w as (PARTITION BY userId)\n",
        "    '''\n",
        ")\n",
        "\n",
        "\n",
        "num_bins = 10\n",
        "\n",
        "# Define a function to calculate histogram bins with probabilities\n",
        "def calculate_histogram_bins_prob(timestamps):\n",
        "    # Calculate bin size\n",
        "    bin_size = 0.1\n",
        "    # Initialize bins\n",
        "    n = len(timestamps)\n",
        "    bins = [0.0]*10\n",
        "    # Iterate over timestamps and increment bin counts\n",
        "    for ts in timestamps:\n",
        "        index = int(ts  // bin_size)\n",
        "        if index>=10:\n",
        "            index = 9\n",
        "        elif index < 0 :\n",
        "            index = 0\n",
        "        bins[index] += 1/n\n",
        "\n",
        "    return bins\n",
        "\n",
        "\n",
        "# Register the UDF\n",
        "histogram_bins_prob_udf = udf(calculate_histogram_bins_prob, ArrayType(DoubleType()))\n",
        "\n",
        "# Apply the UDF to calculate histogram bins with probabilities for each user\n",
        "action_distribution = action_ts.groupBy(\"userId\").agg(histogram_bins_prob_udf(collect_list('ts_percent')).alias(\"histogram_bins_prob\"),\n",
        "                                                      mean('ts_percent').alias('action_ts_avg'),\n",
        "                                                      stddev('ts_percent').alias('action_ts_std')\n",
        "                                                     )\n",
        "\n",
        "# Split list into columns\n",
        "action_distribution = action_distribution.select(\"userId\",\n",
        "                           action_distribution.histogram_bins_prob[0].alias('action_hist_0'),\n",
        "                           action_distribution.histogram_bins_prob[1].alias('action_hist_1'),\n",
        "                           action_distribution.histogram_bins_prob[2].alias('action_hist_2'),\n",
        "                           action_distribution.histogram_bins_prob[3].alias('action_hist_3'),\n",
        "                           action_distribution.histogram_bins_prob[4].alias('action_hist_4'),\n",
        "                           action_distribution.histogram_bins_prob[5].alias('action_hist_5'),\n",
        "                           action_distribution.histogram_bins_prob[6].alias('action_hist_6'),\n",
        "                           action_distribution.histogram_bins_prob[7].alias('action_hist_7'),\n",
        "                           action_distribution.histogram_bins_prob[8].alias('action_hist_8'),\n",
        "                           action_distribution.histogram_bins_prob[9].alias('action_hist_9'),\n",
        "                           \"action_ts_avg\",\"action_ts_std\"\n",
        "                          )\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:33.068105Z",
          "iopub.execute_input": "2024-03-02T16:29:33.068372Z",
          "iopub.status.idle": "2024-03-02T16:29:33.170221Z",
          "shell.execute_reply.started": "2024-03-02T16:29:33.068343Z",
          "shell.execute_reply": "2024-03-02T16:29:33.169204Z"
        },
        "trusted": true,
        "id": "lUDhFXTXSI_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting these new features into the original table"
      ],
      "metadata": {
        "id": "L5GhOtiPSI_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_action = song_cnt.join(song_entropy,on='userId') \\\n",
        "              .join(artist_entropy,on='userId') \\\n",
        "              .join(song_percent,on='userId') \\\n",
        "              .join(action_distribution,on='userId') \\"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:33.171307Z",
          "iopub.execute_input": "2024-03-02T16:29:33.171647Z",
          "iopub.status.idle": "2024-03-02T16:29:33.379790Z",
          "shell.execute_reply.started": "2024-03-02T16:29:33.171611Z",
          "shell.execute_reply": "2024-03-02T16:29:33.378850Z"
        },
        "trusted": true,
        "id": "a8azn5ewSI_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### User-platform Interaction Feature\n",
        "\n",
        "| Feature    | DataType | Description |\n",
        "| --- | ---| ---|\n",
        "|auth|string(categorical)|authentication level (Logged In, Logged Out, Cancelled, Guest)|\n",
        "|itemInSession|long|log count in a given session|\n",
        "|method|string(categorical)|http request method (GET and PUT)|\n",
        "|page|string(categorical)|type of interaction (NextSong, Home, **Cancellation Confirmation**, etc.)|\n",
        "|sessionId|long|session to which the log belongs to|\n",
        "|status|long(categorical)|http status code (200, 307 and 404)|\n",
        "|ts|long|timestamp of a given log|\n",
        "|userAgent|string|agent used by the user to access the streaming service|\n"
      ],
      "metadata": {
        "id": "fiHocaHeSI_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inter_page = spark.sql('''\n",
        "    SELECT UserId,\n",
        "           SUM(IF(page=\"Submit Downgrade\",1,0)) as cnt_page_submitDowngrade,\n",
        "           SUM(IF(page=\"Thumbs Down\",1,0)) as cnt_page_ThumbsDown,\n",
        "           SUM(IF(page=\"Home\",1,0)) as cnt_page_Home,\n",
        "           SUM(IF(page=\"Downgrade\",1,0)) as cnt_page_Downgrade,\n",
        "           SUM(IF(page=\"Roll Advert\",1,0)) as cnt_page_RollAdvert,\n",
        "           SUM(IF(page=\"Logout\",1,0)) as cnt_page_Logout,\n",
        "           SUM(IF(page=\"Save Settings\",1,0)) as cnt_page_SaveSettings,\n",
        "           SUM(IF(page=\"About\",1,0)) as cnt_page_About,\n",
        "           SUM(IF(page=\"Settings\",1,0)) as cnt_page_Settings,\n",
        "           SUM(IF(page=\"Add to Playlist\",1,0)) as cnt_page_AddtoPlaylist,\n",
        "           SUM(IF(page=\"NextSong\",1,0)) as cnt_page_NextSong,\n",
        "           SUM(IF(page=\"Help\",1,0)) as cnt_page_Help,\n",
        "           SUM(IF(page=\"Upgrade\",1,0)) as cnt_page_upgrade,\n",
        "           SUM(IF(page=\"Error\",1,0)) as cnt_page_Error,\n",
        "           SUM(IF(page=\"Submit Upgrade\",1,0)) as cnt_page_SubmitUpgrade\n",
        "    FROM user_log_small_table\n",
        "    GROUP BY userId\n",
        "\n",
        "''')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:33.380808Z",
          "iopub.execute_input": "2024-03-02T16:29:33.381113Z",
          "iopub.status.idle": "2024-03-02T16:29:33.418116Z",
          "shell.execute_reply.started": "2024-03-02T16:29:33.381081Z",
          "shell.execute_reply": "2024-03-02T16:29:33.417125Z"
        },
        "trusted": true,
        "id": "ieY46_vpSI_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inter_session = spark.sql('''\n",
        "    WITH CTE AS(\n",
        "        SELECT userId,\n",
        "               sessionId,\n",
        "               (MAX(ts)-MIN(ts))/(1000*3600*24) as session_duration_day\n",
        "        FROM user_log_small_table\n",
        "        GROUP BY userId,sessionId\n",
        "    )\n",
        "\n",
        "    SELECT userId,\n",
        "           COUNT(DISTINCT sessionId) session_cnt,\n",
        "           AVG(session_duration_day) as avg_session_duration_day\n",
        "    FROM CTE\n",
        "    GROUP BY userId\n",
        "''')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:33.419219Z",
          "iopub.execute_input": "2024-03-02T16:29:33.419520Z",
          "iopub.status.idle": "2024-03-02T16:29:33.453169Z",
          "shell.execute_reply.started": "2024-03-02T16:29:33.419485Z",
          "shell.execute_reply": "2024-03-02T16:29:33.452205Z"
        },
        "trusted": true,
        "id": "ftyILJnpSI_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put everything together"
      ],
      "metadata": {
        "id": "0HzjoyfxSI_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_inter = inter_page.join(inter_session,on='userId')\n",
        "data_final = data_user.join(data_action,on='userId').join(data_inter,on='userId')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:33.454220Z",
          "iopub.execute_input": "2024-03-02T16:29:33.454532Z",
          "iopub.status.idle": "2024-03-02T16:29:33.792325Z",
          "shell.execute_reply.started": "2024-03-02T16:29:33.454500Z",
          "shell.execute_reply": "2024-03-02T16:29:33.791345Z"
        },
        "trusted": true,
        "id": "SbMqnQ9GSI_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_final.printSchema()\n",
        "data_final = data_final.withColumn(\"cnt_page_ThumbsDown\",data_final.cnt_page_ThumbsDown/data_final.registration_duration_day) \\\n",
        "                       .withColumn(\"cnt_page_Home\",data_final.cnt_page_Home/data_final.registration_duration_day) \\\n",
        "                       .withColumn(\"cnt_page_Downgrade\",data_final.cnt_page_Downgrade/data_final.registration_duration_day) \\\n",
        "                       .withColumn(\"cnt_page_RollAdvert\",data_final.cnt_page_RollAdvert/data_final.registration_duration_day) \\\n",
        "                       .withColumn(\"cnt_page_Logout\",data_final.cnt_page_Logout/data_final.registration_duration_day) \\\n",
        "                       .withColumn(\"cnt_page_SaveSettings\",data_final.cnt_page_SaveSettings/data_final.registration_duration_day) \\\n",
        "                       .withColumn(\"cnt_page_About\",data_final.cnt_page_About/data_final.registration_duration_day) \\\n",
        "                       .withColumn(\"cnt_page_Settings\",data_final.cnt_page_Settings/data_final.registration_duration_day) \\\n",
        "                       .withColumn(\"cnt_page_AddtoPlaylist\",data_final.cnt_page_AddtoPlaylist/data_final.registration_duration_day) \\\n",
        "                       .withColumn(\"cnt_page_NextSong\",data_final.cnt_page_NextSong/data_final.registration_duration_day) \\\n",
        "                       .withColumn(\"cnt_page_Help\",data_final.cnt_page_Help/data_final.registration_duration_day) \\\n",
        "                       .withColumn(\"cnt_page_upgrade\",data_final.cnt_page_upgrade/data_final.registration_duration_day) \\\n",
        "                       .withColumn(\"cnt_page_Error\",data_final.cnt_page_Error/data_final.registration_duration_day) \\\n",
        "                       .withColumn(\"cnt_page_SubmitUpgrade\",data_final.cnt_page_SubmitUpgrade/data_final.registration_duration_day)\n",
        "\n",
        "data_final = data_final.drop(\"userId\",\"location\",\"gender\",\"level_free_avg_duration_day\",\"level_paid_avg_duration_day\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:33.793345Z",
          "iopub.execute_input": "2024-03-02T16:29:33.793652Z",
          "iopub.status.idle": "2024-03-02T16:29:34.972921Z",
          "shell.execute_reply.started": "2024-03-02T16:29:33.793621Z",
          "shell.execute_reply": "2024-03-02T16:29:34.972142Z"
        },
        "trusted": true,
        "id": "B_s5S-DNSI_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"eda\"></a>\n",
        "# 2.2.3 Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "SSoEjKkmSI_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.stat import Correlation\n",
        "\n",
        "vec_col = 'corr_features'\n",
        "# assemble all vector columns into one vector column\n",
        "assembler = VectorAssembler(inputCols=data_final.columns, outputCol=vec_col)\n",
        "corr_df = assembler.transform(data_final).select(vec_col)\n",
        "\n",
        "# compute the correlation between 'churn' and every feature and the correlation between each pair of features\n",
        "corr_mat = Correlation.corr(corr_df, vec_col)\n",
        "# convert the corrlation matrix to a pandas dataframe with column names\n",
        "corr_values = corr_mat.collect()[0][0].values\n",
        "corr_mat_pd = pd.DataFrame(corr_values.reshape(-1, len(data_final.columns)), \\\n",
        "                           index=data_final.columns, columns=data_final.columns)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:29:34.973716Z",
          "iopub.execute_input": "2024-03-02T16:29:34.973971Z",
          "iopub.status.idle": "2024-03-02T16:30:04.709019Z",
          "shell.execute_reply.started": "2024-03-02T16:29:34.973946Z",
          "shell.execute_reply": "2024-03-02T16:30:04.707952Z"
        },
        "trusted": true,
        "id": "UPx1J-lsSI_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Draw the corrlation among features"
      ],
      "metadata": {
        "id": "Zz_XYII2SI_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot feature correlations\n",
        "sns.heatmap(corr_mat_pd, vmin=-1, vmax=1, cmap=sns.diverging_palette(240, 20, as_cmap=True),\n",
        "            linewidths=.5, cbar_kws={\"shrink\": .5}, square=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:30:04.712957Z",
          "iopub.execute_input": "2024-03-02T16:30:04.713963Z",
          "iopub.status.idle": "2024-03-02T16:30:05.460016Z",
          "shell.execute_reply.started": "2024-03-02T16:30:04.713921Z",
          "shell.execute_reply": "2024-03-02T16:30:05.459166Z"
        },
        "trusted": true,
        "id": "5kP-H0S7SI_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's delve deeper into the correlation between churn and other features. As anticipated, our feature engineering efforts have proven fruitful, with ***action_ts_avg*** and ***action_ts_std*** emerging as the two features exhibiting the most significant absolute correlation values."
      ],
      "metadata": {
        "id": "oKNW6WjASI_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,1)\n",
        "temp_data = corr_mat_pd['churn'].reset_index().query('index!=\"churn\"').sort_values('churn',ascending=False)\n",
        "sns.barplot(data=temp_data,y='index',x='churn',palette='coolwarm',ax=ax)\n",
        "ax.set_yticklabels(temp_data['index'],fontsize=8)\n",
        "ax.set_ylabel('')\n",
        "ax.set_xlabel('Corrleation with churn')\n",
        "fig.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:30:05.461195Z",
          "iopub.execute_input": "2024-03-02T16:30:05.461580Z",
          "iopub.status.idle": "2024-03-02T16:30:06.063702Z",
          "shell.execute_reply.started": "2024-03-02T16:30:05.461541Z",
          "shell.execute_reply": "2024-03-02T16:30:06.062691Z"
        },
        "trusted": true,
        "id": "R9egZXuCSI_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will conduct two-sample t-tests for churn and non-churn user groups to determine which features exhibit statistically significant differences between the two groups."
      ],
      "metadata": {
        "id": "GlEWYVBOSI_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.multitest import multipletests\n",
        "\n",
        "def ttest(df,group_feature, test_feature):\n",
        "    flag0 = df[group_feature]==0\n",
        "    flag1 = ~flag0\n",
        "    vector_0 = df.loc[flag0,test_feature]\n",
        "    vector_1 = df.loc[flag1,test_feature]\n",
        "    statistic, pvalue = stats.ttest_ind(vector_1, vector_0)\n",
        "    return [statistic,pvalue,test_feature]\n",
        "\n",
        "\n",
        "pd_data_final = data_final.toPandas()\n",
        "ttest_result = []\n",
        "for c in pd_data_final.columns:\n",
        "    result = ttest(pd_data_final,'churn',c)\n",
        "    ttest_result.append(result)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:30:06.065107Z",
          "iopub.execute_input": "2024-03-02T16:30:06.065354Z",
          "iopub.status.idle": "2024-03-02T16:30:27.637041Z",
          "shell.execute_reply.started": "2024-03-02T16:30:06.065328Z",
          "shell.execute_reply": "2024-03-02T16:30:27.636100Z"
        },
        "trusted": true,
        "id": "zD4L6aGkSI_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# p-value adjustment for multiple test\n",
        "df_ttest_result = pd.DataFrame(ttest_result,columns=['stats','pvalue','feature'])\n",
        "df_ttest_result.sort_values('pvalue')\n",
        "df_ttest_result['pvalue_adjust'] = multipletests(df_ttest_result['pvalue'],method='fdr_bh')[1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:30:27.643118Z",
          "iopub.execute_input": "2024-03-02T16:30:27.643652Z",
          "iopub.status.idle": "2024-03-02T16:30:27.651172Z",
          "shell.execute_reply.started": "2024-03-02T16:30:27.643621Z",
          "shell.execute_reply": "2024-03-02T16:30:27.649926Z"
        },
        "trusted": true,
        "id": "u381rk0bSI_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_feature = df_ttest_result.query('pvalue_adjust<0.05 and feature!=\"churn\"').sort_values('stats')['feature'].to_list()\n",
        "df_ttest_result.query('pvalue_adjust<0.05 and feature!=\"churn\"')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:30:27.652032Z",
          "iopub.execute_input": "2024-03-02T16:30:27.652270Z",
          "iopub.status.idle": "2024-03-02T16:30:27.678126Z",
          "shell.execute_reply.started": "2024-03-02T16:30:27.652246Z",
          "shell.execute_reply": "2024-03-02T16:30:27.677207Z"
        },
        "trusted": true,
        "id": "_dWvYeclSI_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(2,4,figsize=(20,10))\n",
        "index = 0\n",
        "for i in range(2):\n",
        "    for j in range(4):\n",
        "        sns.boxplot(x=\"churn\",y=candidate_feature[index],data=pd_data_final,ax=ax[i,j])\n",
        "        ax[i,j].set_title(candidate_feature[index])\n",
        "        ax[i,j].set_ylabel('')\n",
        "        index += 1\n",
        "fig.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:30:27.679128Z",
          "iopub.execute_input": "2024-03-02T16:30:27.679361Z",
          "iopub.status.idle": "2024-03-02T16:30:28.605479Z",
          "shell.execute_reply.started": "2024-03-02T16:30:27.679338Z",
          "shell.execute_reply": "2024-03-02T16:30:28.604607Z"
        },
        "trusted": true,
        "id": "f69u-jf8SI_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***registration_duration_day***  \n",
        "The feature ***registration_duration_day*** exhibited a highly statistically significant difference (p-value_adjust = 0.000011) between churn and non-churn user groups. This suggests that users who have been registered for a longer duration are less likely to churn. If we aim to reduce the churn rate, our focus should be on new users, as they are more likely to churn within a shorter duration of registration.\n",
        "\n",
        "***action_ts_avg***  \n",
        "Regarding ***action_ts_avg*** (the average timestamp of a user's actions normalized by their lifespan), it appears that non-churning users tend to engage with the Sparkify platform earlier after registration compared to churning users.\n",
        "\n",
        "***action_ts_std***  \n",
        "***action_ts_std*** suggests that users with high variability in their usage times are less likely to churn. In essence, users who interact with the platform regularly are more likely to become loyal customers.\n",
        "\n",
        "***action_hist_5*** and ***action_hist_9***  \n",
        "Non-churn users show increased interaction in the middle of their customer lifespan, while churn users exhibit a spike just before churn. This suggests specific pre-churn activities. Further investigation into churn user activities before churn could inform mitigation strategies. The boxplot illustrates action frequency distribution across customer lifespan phases. Understanding these patterns aids in tailoring interventions for improved retention.\n"
      ],
      "metadata": {
        "id": "ZfqNGsK8SI_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd_data_final['id'] = range(pd_data_final.shape[0])\n",
        "\n",
        "df_action = pd.wide_to_long(pd_data_final[['id','churn','action_hist_0','action_hist_1','action_hist_2','action_hist_3','action_hist_4','action_hist_5','action_hist_6','action_hist_7','action_hist_8','action_hist_9']],\n",
        "                stubnames=['action'],\n",
        "                i=['id','churn'],\n",
        "                j= 'period',\n",
        "                sep='_hist_',\n",
        "                suffix='.+'\n",
        "               ).reset_index()\n",
        "\n",
        "fig,ax = plt.subplots(1,1,figsize=(20,10))\n",
        "sns.boxplot(x='period',y='action',data=df_action,hue='churn',ax=ax)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:30:28.606580Z",
          "iopub.execute_input": "2024-03-02T16:30:28.607515Z",
          "iopub.status.idle": "2024-03-02T16:30:29.203610Z",
          "shell.execute_reply.started": "2024-03-02T16:30:28.607488Z",
          "shell.execute_reply": "2024-03-02T16:30:29.202535Z"
        },
        "trusted": true,
        "id": "H5_Me9WGSI_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"fs\"></a>\n",
        "# 2.2.4 Feature Selection"
      ],
      "metadata": {
        "id": "UEHJ8s1ISI_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our feature correlation heatmap revealed numerous features with high correlations. To mitigate collinearity, I will identify groups of closely correlated features and select only one from each group for subsequent modeling. This approach aims to streamline the feature set while preserving the most relevant information for our analysis. By reducing redundancy in correlated features, we can enhance model performance and interpretability while avoiding multicollinearity issues that may arise during modeling.\n"
      ],
      "metadata": {
        "id": "9TeIVG7OSI_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "\n",
        "# construct an adjacency matrix where high correlation (> 0.85) is labeled as 1, otherwise 0\n",
        "is_high_corr = np.abs(corr_mat_pd.values) > 0.85\n",
        "adj_mat = csr_matrix(is_high_corr.astype(int) - np.eye(len(data_final.columns)))\n",
        "\n",
        "# find groups of highly correlated features by finding the connected components in the adjacency matrix\n",
        "_, corr_labels = connected_components(csgraph=adj_mat, directed=False)\n",
        "unique, unique_counts = np.unique(corr_labels, return_counts=True)\n",
        "# get groups with size > 1\n",
        "high_corr_labels = unique[unique_counts > 1]\n",
        "\n",
        "# if there is at least one group of highly correlated features\n",
        "if len(high_corr_labels) > 0:\n",
        "    # map the label indices of highly correlated features to their column names\n",
        "    print('Highly correlated features include:')\n",
        "    high_corr_col_dict = {}\n",
        "    for high_corr_label in high_corr_labels:\n",
        "        high_corr_col_dict[high_corr_label] = [col_name for corr_label, col_name in zip(corr_labels, data_final.columns)\n",
        "                                               if corr_label == high_corr_label]\n",
        "        print(high_corr_col_dict[high_corr_label])\n",
        "\n",
        "    print('\\nFeatures to keep:')\n",
        "    cols_to_drop = []\n",
        "    for col_name_list in high_corr_col_dict.values():\n",
        "        # keep the feature that has the highest correlation with 'Churn'\n",
        "        col_to_keep = corr_mat_pd.loc[col_name_list,'churn'].idxmax()\n",
        "        print(col_to_keep)\n",
        "        # remove the other features to avoid multicollinearity\n",
        "        col_name_list.remove(col_to_keep)\n",
        "        corr_mat_pd.drop(index=col_name_list, columns=col_name_list, inplace=True)\n",
        "        cols_to_drop.extend(col_name_list)\n",
        "\n",
        "data_final = data_final.drop(*cols_to_drop)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:30:29.205294Z",
          "iopub.execute_input": "2024-03-02T16:30:29.205663Z",
          "iopub.status.idle": "2024-03-02T16:30:29.275611Z",
          "shell.execute_reply.started": "2024-03-02T16:30:29.205625Z",
          "shell.execute_reply": "2024-03-02T16:30:29.274920Z"
        },
        "trusted": true,
        "id": "JwxrTO3BSI_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"dm\"></a>\n",
        "# 2.3 Data Modeling"
      ],
      "metadata": {
        "id": "Qmsk2TZJSI_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into train, validation and test sets using stratified sampling based on 'churn'\n",
        "train = data_final.sampleBy('churn', fractions={0: 0.8, 1: 0.8}, seed=42)\n",
        "test = data_final.subtract(train)\n",
        "\n",
        "# assign class weight\n",
        "y_collect = train.select('churn').groupBy('churn').count().collect()\n",
        "bin_counts = {y['churn']: y['count'] for y in y_collect}\n",
        "total = np.sum(e for e in bin_counts.values())\n",
        "n_labels = len(bin_counts)\n",
        "weights = {bin_: total/(n_labels*count) for bin_, count in bin_counts.items()}\n",
        "train = train.withColumn('weight', when(col('churn')==1.0, weights[1]).otherwise(weights[0]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:30:29.276596Z",
          "iopub.execute_input": "2024-03-02T16:30:29.277451Z",
          "iopub.status.idle": "2024-03-02T16:30:41.355623Z",
          "shell.execute_reply.started": "2024-03-02T16:30:29.277416Z",
          "shell.execute_reply": "2024-03-02T16:30:41.354535Z"
        },
        "trusted": true,
        "id": "JkkuupGJSI_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"pipe\"></a>\n",
        "##  2.3.1  Building the Pipeline and Hyperparameter tuning\n"
      ],
      "metadata": {
        "id": "aDiuFS2zSI_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance metrics\n",
        "In selecting evaluation metrics, I opted for AUC-PR and F1 instead of AUC-ROC. AUC-PR specifically assesses classifier performance with emphasis on the positive class, making it particularly advantageous for imbalanced datasets, which is the case here. This choice allows for a more nuanced evaluation that prioritizes model performance in scenarios where correctly identifying positive instances, such as churn cases, is of paramount importance."
      ],
      "metadata": {
        "id": "L9ATrDk-SI_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = VectorAssembler(inputCols=[c for c in train.columns if c not in ['churn','weight']], outputCol=\"NumFeatures\")\n",
        "#scaler = StandardScaler(inputCol=\"NumFeatures\", outputCol=\"ScaledNumFeatures\", withMean=True,withStd=True)\n",
        "scaler = MinMaxScaler(inputCol=\"NumFeatures\", outputCol=\"ScaledNumFeatures\")\n",
        "\n",
        "# logistic regression\n",
        "lr = LogisticRegression(featuresCol='ScaledNumFeatures', weightCol='weight', labelCol='churn')\n",
        "pipeline_lr = Pipeline(stages=[assembler, scaler, lr])\n",
        "paramGrid_lr = ParamGridBuilder() \\\n",
        "            .addGrid(lr.regParam, [0.01, 0.1,0.5],) \\\n",
        "            .addGrid(lr.maxIter, [10, 20,40]) \\\n",
        "             .build()\n",
        "crossval_lr = CrossValidator(estimator = pipeline_lr,\n",
        "                             estimatorParamMaps = paramGrid_lr,\n",
        "                             evaluator = BinaryClassificationEvaluator(labelCol='churn', weightCol='weight',metricName='areaUnderPR'),\n",
        "                             parallelism=4,\n",
        "                             numFolds=4)\n",
        "\n",
        "# random forest\n",
        "rf = RandomForestClassifier(featuresCol='ScaledNumFeatures', weightCol='weight',labelCol='churn', seed=42,)\n",
        "pipeline_rf = Pipeline(stages=[assembler, scaler, rf])\n",
        "paramGrid_rf = ParamGridBuilder() \\\n",
        "              .addGrid(rf.numTrees, [50, 100, 150]) \\\n",
        "              .addGrid(rf.maxDepth, [10, 20]) \\\n",
        "              .build()\n",
        "crossval_rf = CrossValidator(estimator = pipeline_rf,\n",
        "                             estimatorParamMaps = paramGrid_rf,\n",
        "                             evaluator = BinaryClassificationEvaluator(labelCol='churn', weightCol='weight',metricName='areaUnderPR'),\n",
        "                             parallelism=4,\n",
        "                             numFolds=4)\n",
        "\n",
        "# SVM\n",
        "svm = LinearSVC( featuresCol=\"ScaledNumFeatures\",weightCol='weight',labelCol=\"churn\")\n",
        "pipeline_svm = Pipeline(stages=[assembler, scaler, svm])\n",
        "paramGrid_svm = ParamGridBuilder() \\\n",
        "    .addGrid(svm.maxIter, [10, 100, 1000]) \\\n",
        "    .addGrid(svm.regParam, [0.1, 0.01]) \\\n",
        "    .build()\n",
        "crossval_svm = CrossValidator(estimator=pipeline_svm,\n",
        "                              estimatorParamMaps=paramGrid_svm,\n",
        "                             evaluator = BinaryClassificationEvaluator(labelCol='churn', weightCol='weight',metricName='areaUnderPR'),\n",
        "                              parallelism=4,\n",
        "                              numFolds=4)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:35:47.164414Z",
          "iopub.execute_input": "2024-03-02T16:35:47.165325Z",
          "iopub.status.idle": "2024-03-02T16:35:47.255511Z",
          "shell.execute_reply.started": "2024-03-02T16:35:47.165290Z",
          "shell.execute_reply": "2024-03-02T16:35:47.254552Z"
        },
        "trusted": true,
        "id": "mrvjq_88SI_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_metrics(pred, label='churn'):\n",
        "    \"\"\"Print evaluation metrics on a test set\n",
        "\n",
        "    Args:\n",
        "    pred: (spark dataframe) a test set\n",
        "\n",
        "    Returns:\n",
        "    summary: (pandas dataframe) a summary of evaluation metrics\n",
        "    \"\"\"\n",
        "    eval_metrics = {}\n",
        "\n",
        "    # compute area under PR curve\n",
        "    evaluator = BinaryClassificationEvaluator(labelCol=label)\n",
        "    auc_pr = evaluator.evaluate(pred, {evaluator.metricName:'areaUnderPR'})\n",
        "    # compute precision, recall and f1 score\n",
        "    predictionAndLabels = pred.select('prediction', label)\n",
        "    # both 'prediction' and label in predictionAndLabels need to be cast to float type and\n",
        "    # map to tuple before calling 'MulticlassMetrics'\n",
        "    metrics = MulticlassMetrics(predictionAndLabels.rdd.map(lambda x: tuple(map(float, x))))\n",
        "\n",
        "    # get overall statistics\n",
        "    eval_metrics['overall'] = [metrics.weightedPrecision, metrics.weightedRecall, \\\n",
        "                               metrics.weightedFMeasure(), auc_pr]\n",
        "\n",
        "    # get statistics by class\n",
        "    classes = [0.0, 1.0]\n",
        "    for cls in classes:\n",
        "        eval_metrics['class ' + str(int(cls))] = [metrics.precision(cls), metrics.recall(cls), \\\n",
        "                                                  metrics.fMeasure(cls), '']\n",
        "\n",
        "    # convert to a pandas dataframe for display\n",
        "    summary = pd.DataFrame.from_dict(eval_metrics, orient='index', \\\n",
        "                                     columns=['precision', 'recall', 'f1 score', 'AUC-PR'])\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T17:27:45.168462Z",
          "iopub.execute_input": "2024-03-02T17:27:45.168928Z",
          "iopub.status.idle": "2024-03-02T17:27:45.182693Z",
          "shell.execute_reply.started": "2024-03-02T17:27:45.168886Z",
          "shell.execute_reply": "2024-03-02T17:27:45.181474Z"
        },
        "trusted": true,
        "id": "R_jXmyMLSI_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-02T15:55:32.121636Z",
          "iopub.status.busy": "2024-03-02T15:55:32.121401Z",
          "iopub.status.idle": "2024-03-02T16:08:43.885569Z",
          "shell.execute_reply": "2024-03-02T16:08:43.884862Z",
          "shell.execute_reply.started": "2024-03-02T15:55:32.121599Z"
        },
        "tags": [],
        "id": "4e2a5944-ff12-4a37-85ba-1e1edee2eb84",
        "outputId": "c8922ea5-cc6e-4e36-f239-bc49cb9104f7",
        "colab": {
          "referenced_widgets": [
            "a0535e3685f04302b92e9a32627666eb",
            ""
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0535e3685f04302b92e9a32627666eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception in thread cell_monitor-13:\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/notebook-env/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/mnt/notebook-env/lib/python3.9/threading.py\", line 917, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/mnt/notebook-env/lib/python3.9/site-packages/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
            "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
            "  File \"/mnt/notebook-env/lib/python3.9/site-packages/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
            "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
            "KeyError: 'jobGroup'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         precision    recall  f1 score   AUC-PR\n",
            "overall   0.863429  0.857143  0.859502  0.72207\n",
            "class 0   0.920000  0.884615  0.901961         \n",
            "class 1   0.700000  0.777778  0.736842         \n",
            "/mnt/yarn/usercache/livy/appcache/application_1709394439555_0001/container_1709394439555_0001_01_000001/pyspark.zip/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead."
          ]
        }
      ],
      "source": [
        "cv_lr = crossval_lr.fit(train)\n",
        "test_prediction = cv_lr.transform(test)\n",
        "print_metrics(test_prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-02T16:08:43.886565Z",
          "iopub.status.busy": "2024-03-02T16:08:43.886405Z",
          "iopub.status.idle": "2024-03-02T16:21:17.459983Z",
          "shell.execute_reply": "2024-03-02T16:21:17.459306Z",
          "shell.execute_reply.started": "2024-03-02T16:08:43.886544Z"
        },
        "tags": [],
        "id": "f167d356-574b-4e99-8b8f-7a59042d8aff",
        "outputId": "098dc7ae-f14f-4f0f-e97a-6e4abb672968",
        "colab": {
          "referenced_widgets": [
            "9c65e4e7b22a44d1b75ff184f85dd9e2",
            ""
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c65e4e7b22a44d1b75ff184f85dd9e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception in thread cell_monitor-14:\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/notebook-env/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/mnt/notebook-env/lib/python3.9/threading.py\", line 917, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/mnt/notebook-env/lib/python3.9/site-packages/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
            "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
            "  File \"/mnt/notebook-env/lib/python3.9/site-packages/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
            "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
            "KeyError: 'jobGroup'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         precision    recall  f1 score    AUC-PR\n",
            "overall   0.972487  0.971429  0.970858  0.966232\n",
            "class 0   0.962963  1.000000  0.981132          \n",
            "class 1   1.000000  0.888889  0.941176          \n",
            "/mnt/yarn/usercache/livy/appcache/application_1709394439555_0001/container_1709394439555_0001_01_000001/pyspark.zip/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead."
          ]
        }
      ],
      "source": [
        "cv_rf = crossval_rf.fit(train)\n",
        "test_prediction = cv_rf.transform(test)\n",
        "print_metrics(test_prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-02T16:21:17.461091Z",
          "iopub.status.busy": "2024-03-02T16:21:17.460924Z",
          "iopub.status.idle": "2024-03-02T17:35:07.547775Z",
          "shell.execute_reply": "2024-03-02T17:35:07.547173Z",
          "shell.execute_reply.started": "2024-03-02T16:21:17.461068Z"
        },
        "tags": [],
        "id": "fe8c8cdf-32b0-42f0-8943-8e91f3c35b17",
        "outputId": "6412952b-dd07-4639-f28c-7eef2f3e03f9",
        "colab": {
          "referenced_widgets": [
            "4ea8eec5285b47e7b4619632d31526d3",
            ""
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ea8eec5285b47e7b4619632d31526d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception in thread cell_monitor-15:\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/notebook-env/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/mnt/notebook-env/lib/python3.9/threading.py\", line 917, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/mnt/notebook-env/lib/python3.9/site-packages/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
            "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
            "  File \"/mnt/notebook-env/lib/python3.9/site-packages/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
            "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
            "KeyError: 'jobGroup'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         precision    recall  f1 score    AUC-PR\n",
            "overall   0.863429  0.857143  0.859502  0.733718\n",
            "class 0   0.920000  0.884615  0.901961          \n",
            "class 1   0.700000  0.777778  0.736842          \n",
            "/mnt/yarn/usercache/livy/appcache/application_1709394439555_0001/container_1709394439555_0001_01_000001/pyspark.zip/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead."
          ]
        }
      ],
      "source": [
        "cv_svm = crossval_svm.fit(train)\n",
        "test_prediction = cv_svm.transform(test)\n",
        "print_metrics(test_prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"conclusion\"></a>\n",
        "# 3. Conclusion\n"
      ],
      "metadata": {
        "id": "Hq-Ng1lcSI_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Algorithm    | Best Parameter | <td colspan=2> **Precision** | <td colspan=2> **Recall**| <td colspan=2> **F1-Score** | AUC-ROC | AUC-PR |\n",
        "| --- | --- | --- | --- | --- | --- | --- |\n",
        "|     |  | <td colspan=1> Overall <td colspan=1> Churned | <td colspan=1> Overall <td colspan=1> Churned| <td colspan=1> Overall <td colspan=1> Churned | |  |\n",
        "|Logistic Regression|RegParam=0.01, ElasticNetParam=0.5, MaxIter=20|<td colspan=1>0.81 <td colspan=1>0.53|<td colspan=1>0.77 <td colspan=1>0.77|<td colspan=1>0.78 <td colspan=1>0.63|0.80|0.85|\n",
        "|Random Forest|NumTrees=100, MaxDepth=15|<td colspan=1>0.84 <td colspan=1>1.00|<td colspan=1>0.80 <td colspan=1>0.22|<td colspan=1>0.74 <td colspan=1>0.36|0.69|0.83|\n",
        "    \n",
        "    \n",
        " Based on the solid performance on the mini dataset, we decide to use logistic regression classifier for future training and examine the feature importance in this model."
      ],
      "metadata": {
        "id": "n6anxeQySI_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importances = [coef for coef in cv_lr.bestModel.stages[-1].coefficients]\n",
        "feat_imp_pd = pd.DataFrame({'feature': features, 'importance': importances}).sort_values('importance', ascending = False)\n",
        "\n",
        "# plot feature importance\n",
        "sns.barplot(data=feat_imp_pd, y=\"feature\", x=\"importance\", palette='coolwarm', zorder=2)\n",
        "plt.grid(axis='x', linestyle='--', zorder=0)\n",
        "plt.ylabel('');"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-02T16:30:41.381168Z",
          "iopub.status.idle": "2024-03-02T16:30:41.381927Z",
          "shell.execute_reply.started": "2024-03-02T16:30:41.381627Z",
          "shell.execute_reply": "2024-03-02T16:30:41.381653Z"
        },
        "trusted": true,
        "id": "tQGn5DWQSI_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Obseration  \n",
        "The positive impact of increased interactions with ***ThumbsDown*** and ***RollAdvert*** pages on user churn is intuitive. However, noteworthy features include ***SubmitUpgrade*** and ***SaveSettings***. It's conceivable that churn users were dissatisfied with their current settings or account configuration, leading them to attempt changes through settings adjustments or account upgrades. Additionally, the high frequency of advertisements between songs may have been particularly bothersome, potentially contributing to churn. This annoyance could also be related to users' difficulties in finding songs or artists they enjoy, as suggested by ***artist_entropy*** and ***song_percent_std***. Rather than sticking to specific artists and listening to songs in full, churn users may have been frantically browsing songs in hopes of discovering ones they like.\n",
        "\n",
        "### Busines Impact  \n",
        "Deploying the predictive model enables Sparkify to proactively identify potential churners among its user base. By offering attractive perks and incentives to these users, Sparkify can effectively reduce churn rates and bolster revenue streams.  \n",
        "\n",
        "Moreover, the analysis yields valuable business insights that inform strategic improvements:  \n",
        "\n",
        "**Enhancing Sparkify's Settings**  \n",
        "Delving into the settings adjustments attempted by churn users offers crucial insights into their dissatisfaction. Understanding the specific settings they sought to change provides actionable intelligence for enhancing user experiences.  \n",
        "\n",
        "**Optimizing the Recommendation System**    \n",
        "  Exploring churn users' favorite songs or artists sheds light on their preferences and unveils opportunities for refining the recommendation system. Addressing the system's cold start problem by collecting more user preference information upfront can elevate user satisfaction and retention. Additionally, if desired content is absent from Sparkify's catalog, exploring avenues to acquire copyright licenses may be necessary to meet user expectations and curb churn rates.  \n",
        "\n",
        "### Future Work\n",
        "**Advanced Feature Engineering**  \n",
        "Introducing more sophisticated feature engineering techniques can enhance the predictive power of the model. For instance, incorporating time-series related features such as auto-correlation can help identify patterns in user behavior, such as regular usage patterns of Sparkify.\n",
        "\n",
        "**Early Churn Prediction**  \n",
        "While our current study focuses on predicting churn using churned customer's full lifespan data, there's potential to develop models for early churn prediction. We'll focus on early churn prediction by truncating churned customer's late-stage lifespan data. This allows us to detect churn behavior earlier, enabling proactive retention strategies.\n"
      ],
      "metadata": {
        "id": "b215yeu9SI_w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X4YxrJioSI_w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}