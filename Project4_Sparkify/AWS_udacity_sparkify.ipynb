{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aac74178-7401-46a7-9603-ffe0e4748a8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:51:32.813446Z",
     "iopub.status.busy": "2024-03-02T15:51:32.813122Z",
     "iopub.status.idle": "2024-03-02T15:53:11.836261Z",
     "shell.execute_reply": "2024-03-02T15:53:11.835525Z",
     "shell.execute_reply.started": "2024-03-02T15:51:32.813410Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd5029177ed498aa26e4229b5c6ecd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1709394439555_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-31-86.us-east-2.compute.internal:20888/proxy/application_1709394439555_0001/\" class=\"emr-proxy-link j-M81IHCJT43UE application_1709394439555_0001\" emr-resource=\"j-M81IHCJT43UE\n\" application-id=\"application_1709394439555_0001\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-26-254.us-east-2.compute.internal:8042/node/containerlogs/container_1709394439555_0001_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Collecting python-dateutil>=2.8.2\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Collecting numpy<2,>=1.22.4\n",
      "  Downloading numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.13.0)\n",
      "Installing collected packages: tzdata, python-dateutil, numpy, pandas\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.1\n",
      "    Not uninstalling python-dateutil at /usr/lib/python3.9/site-packages, outside environment /mnt/yarn/usercache/livy/appcache/application_1709394439555_0001/container_1709394439555_0001_01_000001/tmp/spark-b5d0e550-c230-4f33-8fa5-2f4d934a0e50\n",
      "    Can't uninstall 'python-dateutil'. No files were found to uninstall.\n",
      "Successfully installed numpy-1.26.4 pandas-2.2.1 python-dateutil-2.9.0.post0 tzdata-2024.1\n",
      "\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\n",
      "Collecting pillow>=8\n",
      "  Downloading pillow-10.2.0-cp39-cp39-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.49.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Downloading importlib_resources-6.1.2-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in ./tmp/spark-b5d0e550-c230-4f33-8fa5-2f4d934a0e50/lib64/python3.9/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./tmp/spark-b5d0e550-c230-4f33-8fa5-2f4d934a0e50/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Collecting zipp>=3.1.0\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.13.0)\n",
      "Installing collected packages: zipp, pyparsing, pillow, packaging, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.49.0 importlib-resources-6.1.2 kiwisolver-1.4.5 matplotlib-3.8.3 packaging-23.2 pillow-10.2.0 pyparsing-3.1.1 zipp-3.17.0\n",
      "\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in ./tmp/spark-b5d0e550-c230-4f33-8fa5-2f4d934a0e50/lib64/python3.9/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in ./tmp/spark-b5d0e550-c230-4f33-8fa5-2f4d934a0e50/lib64/python3.9/site-packages (from seaborn) (2.2.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in ./tmp/spark-b5d0e550-c230-4f33-8fa5-2f4d934a0e50/lib64/python3.9/site-packages (from seaborn) (3.8.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./tmp/spark-b5d0e550-c230-4f33-8fa5-2f4d934a0e50/lib64/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./tmp/spark-b5d0e550-c230-4f33-8fa5-2f4d934a0e50/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./tmp/spark-b5d0e550-c230-4f33-8fa5-2f4d934a0e50/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./tmp/spark-b5d0e550-c230-4f33-8fa5-2f4d934a0e50/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./tmp/spark-b5d0e550-c230-4f33-8fa5-2f4d934a0e50/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in ./tmp/spark-b5d0e550-c230-4f33-8fa5-2f4d934a0e50/lib64/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./tmp/spark-b5d0e550-c230-4f33-8fa5-2f4d934a0e50/lib64/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./tmp/spark-b5d0e550-c230-4f33-8fa5-2f4d934a0e50/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./tmp/spark-b5d0e550-c230-4f33-8fa5-2f4d934a0e50/lib64/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.49.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./tmp/spark-b5d0e550-c230-4f33-8fa5-2f4d934a0e50/lib/python3.9/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./tmp/spark-b5d0e550-c230-4f33-8fa5-2f4d934a0e50/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.4->seaborn) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.13.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.5 MB)\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in ./tmp/spark-b5d0e550-c230-4f33-8fa5-2f4d934a0e50/lib64/python3.9/site-packages (from scipy) (1.26.4)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.12.0\n",
      "\n",
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag."
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"pandas\", \"https://pypi.org/simple\")\n",
    "sc.install_pypi_package(\"matplotlib\", \"https://pypi.org/simple\")\n",
    "sc.install_pypi_package(\"seaborn\", \"https://pypi.org/simple\")\n",
    "sc.install_pypi_package(\"scipy\", \"https://pypi.org/simple\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import DoubleType\n",
    "import math\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StandardScaler,MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier,LinearSVC\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql import SparkSession\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "## Create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "## Read in sparkify dataset for EDA\n",
    "#event_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "event_data = \"s3n://udacity-dsnd/sparkify/mini_sparkify_event_data.json\"\n",
    "user_log_small = spark.read.json(event_data)\n",
    "user_log_small.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21bfedb3-8e15-4cff-9c7e-75e7334f1fcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:53:11.837601Z",
     "iopub.status.busy": "2024-03-02T15:53:11.837436Z",
     "iopub.status.idle": "2024-03-02T15:53:13.134025Z",
     "shell.execute_reply": "2024-03-02T15:53:13.133419Z",
     "shell.execute_reply.started": "2024-03-02T15:53:11.837579Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7bf888126b44504be92aeba9eb96f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate churn feature\n",
    "user_log_small.createOrReplaceTempView('user_log_small_table')\n",
    "user_log_small = user_log_small.na.drop(subset=['firstName'])\n",
    "\n",
    "user_churn = spark.sql('''\n",
    "        SELECT userId,\n",
    "               MAX(IF(page = \"Cancellation Confirmation\",1,0)) as churn\n",
    "        FROM user_log_small_table\n",
    "        GROUP BY userId\n",
    "''')\n",
    "\n",
    "# Generate features related to user gender:\n",
    "# Feature 1: gender_cate - Mapping gender values: 'M' to 1, 'F' to 0.\n",
    "\n",
    "user_gender = spark.sql('''\n",
    "    SELECT userId,\n",
    "           SUBSTRING_INDEX(ANY_VALUE(location),', ',-1) as location,\n",
    "           ANY_VALUE(gender) as gender,\n",
    "           IF(ANY_VALUE(gender)='M',1,0) as gender_code\n",
    "    FROM user_log_small_table\n",
    "    GROUP BY userId\n",
    "    ''')\n",
    "\n",
    "# Generate features related to user registration:\n",
    "# Feature 1: registration_duration_day - The length of time since user's registration (unit: days).\n",
    "\n",
    "user_duration = spark.sql('''\n",
    "    SELECT  userID,\n",
    "            ROUND((MAX(ts) - MIN(registration))/(1000*3600*24),2) as registration_duration_day\n",
    "    FROM user_log_small_table\n",
    "    GROUP BY userId\n",
    "''')\n",
    "\n",
    "# Generate features related to user account level transitions and durations:\n",
    "# Feature 1: free_to_paid - Number of transitions from free to paid level\n",
    "# Feature 2: paid_to_free - Number of transitions from paid to free level\n",
    "# Feature 3: level_free_avg_duration_day - Average duration of time the user account remains in the free level (unit: days)\n",
    "# Feature 4: level_paid_avg_duration_day - Average duration of time the user account remains in the paid level (unit: days)\n",
    "# Feature 5: last_level - The user's lastest account level.\n",
    "# Feature 6: duration_beforeFirstAction - Duration of time between user registration and the first platform interaction (unit: days)\n",
    "\n",
    "user_level_switch = spark.sql('''\n",
    "    WITH CTE AS(\n",
    "        SELECT userId,\n",
    "               IF(level='paid' and LAG(level) over w='free',1,0) AS free_to_paid,\n",
    "               IF(level='free' and LAG(level) over w='paid',1,0) AS paid_to_free ,    \n",
    "               IF(LAST(level) over w='paid',1,0) as last_level\n",
    "        FROM user_log_small_table    \n",
    "        WINDOW w as (PARTITION BY userId ORDER BY ts)\n",
    "    )\n",
    "    \n",
    "    SELECT userId,\n",
    "           SUM(free_to_paid) as level_free_to_paid,\n",
    "           SUM(paid_to_free) as level_paid_to_free,\n",
    "           ANY_VALUE(last_level) as last_level\n",
    "    FROM CTE\n",
    "    GROUP BY userId    \n",
    "''')\n",
    "\n",
    "user_level_duration = spark.sql('''\n",
    "    WITH CTE AS(\n",
    "        SELECT userId,\n",
    "               IF(level != LAG(level) over w,1,0) AS change_flag,\n",
    "               level,\n",
    "               ts,\n",
    "               registration\n",
    "        FROM user_log_small_table    \n",
    "        WINDOW w as (PARTITION BY userId ORDER BY ts)\n",
    "    ),\n",
    "    \n",
    "    CTE_phase AS(\n",
    "        SELECT userId,\n",
    "               SUM(change_flag) over w as phase, \n",
    "               level,\n",
    "               ts,\n",
    "               registration\n",
    "        FROM CTE\n",
    "        WINDOW w as (PARTITION BY userId ORDER BY ts ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\n",
    "    ),\n",
    "    \n",
    "    CTE_phase_duration AS(\n",
    "        SELECT userId,\n",
    "               level,\n",
    "               CASE WHEN phase > 0 THEN ROUND((MAX(ts) - MIN(ts))/(1000*3600*24),2) \n",
    "                    WHEN phase = 0 THEN ROUND((MAX(ts) - ANY_VALUE(registration))/(1000*3600*24),2) \n",
    "                    END as duration_day ,\n",
    "               IF(phase = 0,ROUND((MIN(ts) - ANY_VALUE(registration))/(1000*3600*24),2),0) as duration_beforeFirstAction\n",
    "        FROM CTE_phase\n",
    "        GROUP BY userId,level,phase     \n",
    "    ),\n",
    "    \n",
    "    RESULT AS(\n",
    "        SELECT userId,\n",
    "               level,\n",
    "               COUNT(*) as phase_cnt,\n",
    "               ROUND(AVG(duration_day),2) as level_avg_duration_day ,\n",
    "               SUM(duration_beforeFirstAction) as duration_beforeFirstAction\n",
    "        FROM CTE_phase_duration\n",
    "        GROUP BY userId,level\n",
    "    )\n",
    "    \n",
    "    SELECT userId,\n",
    "           MAX(IF(level=\"free\",phase_cnt,0)) as free_cnt,\n",
    "           MAX(IF(level=\"paid\",phase_cnt,0)) as paid_cnt,\n",
    "           MAX(IF(level=\"free\",level_avg_duration_day,0)) as level_free_avg_duration_day,\n",
    "           MAX(IF(level=\"paid\",level_avg_duration_day,0)) as level_paid_avg_duration_day,\n",
    "           MAX(duration_beforeFirstAction) as duration_beforeFirstAction\n",
    "    FROM RESULT\n",
    "    GROUP BY userId\n",
    "    \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a867eb39-da41-4a13-b55e-698d30410471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:53:13.135164Z",
     "iopub.status.busy": "2024-03-02T15:53:13.134992Z",
     "iopub.status.idle": "2024-03-02T15:53:16.440968Z",
     "shell.execute_reply": "2024-03-02T15:53:16.440153Z",
     "shell.execute_reply.started": "2024-03-02T15:53:13.135140Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbbc256ff5e4edabfd45a263a5757de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Genreate feature for location\n",
    "# Feature 1: location_income - The average house-hold income of the location\n",
    "spark_state = spark.read.csv(\"s3://spark-udacity-joo/states.csv\", header=True)\n",
    "#spark_state = spark.createDataFrame(state)\n",
    "spark_state.createOrReplaceTempView('state_table')\n",
    "getLocation = udf(lambda x: x.split(\", \")[-1])\n",
    "\n",
    "user_log_small = user_log_small.withColumn('location',\n",
    "                                           getLocation(user_log_small.location))\n",
    "user_log_small.createOrReplaceTempView('user_log_small_table')\n",
    "\n",
    "location_income = spark.sql('''\n",
    "    WITH log_state AS(\n",
    "        SELECT DISTINCT SUBSTRING_INDEX(LOCATION,', ',-1) as state_abb\n",
    "        FROM user_log_small_table\n",
    "    ),\n",
    "\n",
    "    State_Income AS (\n",
    "        SELECT Abbreviation,\n",
    "               DOUBLE(REPLACE(Income,\",\",\"\")) as Income\n",
    "        FROM  state_table\n",
    "    )\n",
    "\n",
    "    SELECT state_abb as location,\n",
    "           AVG(Income) location_income\n",
    "    FROM log_state\n",
    "         JOIN State_Income ON state_abb = Abbreviation\n",
    "                           OR SUBSTRING_INDEX(SUBSTRING_INDEX(state_abb,'-',1),'-',-1) = Abbreviation\n",
    "                           OR SUBSTRING_INDEX(SUBSTRING_INDEX(state_abb,'-',2),'-',-1) = Abbreviation\n",
    "                           OR SUBSTRING_INDEX(SUBSTRING_INDEX(state_abb,'-',3),'-',-1) = Abbreviation\n",
    "                           OR SUBSTRING_INDEX(SUBSTRING_INDEX(state_abb,'-',4),'-',-1) = Abbreviation\n",
    "    GROUP BY state_abb\n",
    "    ''')\n",
    "\n",
    "data_user = user_churn.join(user_gender,on='userId') \\\n",
    "                  .join(user_duration,on='userId') \\\n",
    "                  .join(user_level_switch,on='userId') \\\n",
    "                  .join(user_level_duration,on='userId') \\\n",
    "                  .join(location_income,on='location')\n",
    "\n",
    "data_user = data_user.withColumn('level_free_avg_duration_ratio',\n",
    "                                 (data_user.level_free_avg_duration_day) /\n",
    "                                 (data_user.registration_duration_day))\n",
    "data_user = data_user.withColumn(\n",
    "    'level_paid_avg_duration_ratio',\n",
    "    data_user.level_paid_avg_duration_day / data_user.registration_duration_day)\n",
    "data_user = data_user.withColumn(\n",
    "    'level_free_duration_ratio', data_user.level_free_avg_duration_day *\n",
    "    data_user.free_cnt / data_user.registration_duration_day)\n",
    "data_user = data_user.withColumn(\n",
    "    'level_paid_duration_ratio', data_user.level_paid_avg_duration_day *\n",
    "    data_user.paid_cnt / data_user.registration_duration_day)\n",
    "\n",
    "\n",
    "user_log_small.createOrReplaceTempView('user_log_small_table')\n",
    "\n",
    "# Genreate feature for song\n",
    "# Feature 1: song_cnt - The number of song listened by user\n",
    "# Feature 2: distinct_song_cnt - The number of unique song listened by user\n",
    "\n",
    "song_cnt = spark.sql('''     \n",
    "    SELECT userId,          \n",
    "           COUNT(song) as song_cnt,\n",
    "           COUNT(DISTINCT song) as distinct_song_cnt\n",
    "    FROM user_log_small_table\n",
    "    WHERE song IS NOT NULL\n",
    "    GROUP BY userId\n",
    "\n",
    "''')\n",
    "# Genreate feature for song\n",
    "# Feature 1: song_entropy - The entropy of the probability distribution of played songs by user\n",
    "# Feature 2: artist_entropy - The entropy of the probability distribution of the artists of played songs by user\n",
    "\n",
    "def entropy(probabilities):\n",
    "  \"\"\"Computes entropy of a list of probabilities.\n",
    "\n",
    "  Args:\n",
    "    probabilities: A list of probabilities.\n",
    "\n",
    "  Returns:\n",
    "    The entropy of the list of probabilities.\n",
    "  \"\"\"\n",
    "\n",
    "  entropy = 0.0\n",
    "  \n",
    "  for probability in probabilities: \n",
    "    entropy -= probability * math.log2(probability)\n",
    "  return entropy\n",
    "\n",
    "\n",
    "entropy_udf = udf(entropy, DoubleType())\n",
    "\n",
    "\n",
    "song_proba = spark.sql(''' \n",
    "    WITH CTE AS(\n",
    "        SELECT userId,\n",
    "               song,\n",
    "               COUNT(*) as song_cnt\n",
    "        FROM user_log_small_table\n",
    "        WHERE song IS NOT NULL\n",
    "        GROUP BY userId,song\n",
    "    ),\n",
    "    \n",
    "    total_table AS(\n",
    "        SELECT userId,\n",
    "               COUNT(*) as total_song_cnt\n",
    "        FROM user_log_small_table\n",
    "        WHERE song IS NOT NULL\n",
    "        GROUP BY userId\n",
    "    )\n",
    "    \n",
    "    SELECT userId,\n",
    "           song,\n",
    "           song_cnt / total_song_cnt as proba\n",
    "    FROM CTE \n",
    "         JOIN total_table USING(userId)\n",
    "    \n",
    "''')\n",
    "\n",
    "artist_proba = spark.sql(''' \n",
    "    WITH CTE AS(\n",
    "        SELECT userId,\n",
    "               artist,\n",
    "               COUNT(*) as artist_cnt\n",
    "        FROM user_log_small_table\n",
    "        WHERE artist IS NOT NULL\n",
    "        GROUP BY userId,artist\n",
    "    ),\n",
    "    \n",
    "    total_table AS(\n",
    "        SELECT userId,\n",
    "               COUNT(*) as total_artist_cnt\n",
    "        FROM user_log_small_table\n",
    "        WHERE artist IS NOT NULL\n",
    "        GROUP BY userId\n",
    "    )\n",
    "    \n",
    "    SELECT userId,\n",
    "           artist,\n",
    "           artist_cnt / total_artist_cnt as proba\n",
    "    FROM CTE \n",
    "         JOIN total_table USING(userId)    \n",
    "''')\n",
    "\n",
    "\n",
    "\n",
    "song_entropy = song_proba.groupBy('userId').agg(entropy_udf(collect_list('proba')).alias('song_entropy'))\n",
    "artist_entropy = artist_proba.groupBy('userId').agg(entropy_udf(collect_list('proba')).alias('artist_entropy'))\n",
    "# Genreate feature for song listening behavior \n",
    "# Feature 1: song_percent_avg - The mean value of percentage of a song listened by user.\n",
    "# Feature 2: song_percent_std - The standard deviation value of percentage of a song listened by user.\n",
    "\n",
    "song_percent = spark.sql(''' \n",
    "        WITH CTE AS(\n",
    "            SELECT userId,\n",
    "                   CASE WHEN ROUND(100*((LEAD(ts) over w ) - ts) /(1000*length),2) > 100 THEN 100\n",
    "                        ELSE ROUND(100*((LEAD(ts) over w ) - ts) /(1000*length),2)\n",
    "                   END as song_percentage\n",
    "            FROM user_log_small_table         \n",
    "            WINDOW w as (PARTITION BY userId, sessionId ORDER BY ts)\n",
    "        )\n",
    "        \n",
    "        SELECT userId,\n",
    "               AVG(song_percentage) as song_percent_avg,\n",
    "               STD(song_percentage) as song_percent_std\n",
    "        FROM CTE\n",
    "        GROUP BY userId\n",
    "    '''\n",
    ")\n",
    "# Genreate feature for the timestamp of user's action\n",
    "# Feature 1: action_ts_avg - Represents the average timing of user actions,\n",
    "#                            It is computed as the mean value of the user action's normalized timestamps.\n",
    "# Feature 2: action_ts_std - Represents the variability in the timing of user actions.\n",
    "#                            It is computed as the standard deviation of the user action's normalized timestamps.\n",
    "# Feature 3: action_hist_0 to action_hist_9 - Represents the distribution of timestamp of user actions.\n",
    "#                                            Each feature (action_hist_i) corresponds to a bin in the histogram of normalized timestamps.\n",
    "#                                            The values represent the frequency of user actions falling into each bin.\n",
    "#All metrics are based on the normalized value between 0 and 1 according to the lifespan of the user account.\n",
    "\n",
    "\n",
    "\n",
    "action_ts = spark.sql(''' \n",
    "       \n",
    "        SELECT userId,\n",
    "               (ts -  (MIN(registration) over w)) / ((MAX(ts) over w) - (MIN(registration) over w)) as ts_percent  \n",
    "        FROM user_log_small_table        \n",
    "        WINDOW w as (PARTITION BY userId)\n",
    "        \n",
    "    '''\n",
    ")\n",
    "\n",
    "\n",
    "num_bins = 10\n",
    "\n",
    "# Define a function to calculate histogram bins with probabilities\n",
    "def calculate_histogram_bins_prob(timestamps):\n",
    "    \n",
    "    # Calculate bin size\n",
    "    bin_size = 0.1\n",
    "    # Initialize bins\n",
    "\n",
    "    n = len(timestamps)\n",
    "    bins = [0.0]*10\n",
    "    # Iterate over timestamps and increment bin counts\n",
    "    for ts in timestamps:\n",
    "        index = int(ts  // bin_size)        \n",
    "        if index>=10:\n",
    "            index = 9\n",
    "        elif index < 0 :\n",
    "            index = 0 \n",
    "        bins[index] += 1/n\n",
    "   \n",
    "    return bins\n",
    "\n",
    "\n",
    "# Register the UDF\n",
    "histogram_bins_prob_udf = udf(calculate_histogram_bins_prob, ArrayType(DoubleType()))\n",
    "\n",
    "\n",
    "# Apply the UDF to calculate histogram bins with probabilities for each user\n",
    "action_distribution = action_ts.groupBy(\"userId\").agg(histogram_bins_prob_udf(collect_list('ts_percent')).alias(\"histogram_bins_prob\"),\n",
    "                                                      mean('ts_percent').alias('action_ts_avg'),\n",
    "                                                      stddev('ts_percent').alias('action_ts_std')\n",
    "                                                     )\n",
    "\n",
    "# Split list into columns \n",
    "action_distribution = action_distribution.select(\"userId\",\n",
    "                           action_distribution.histogram_bins_prob[0].alias('action_hist_0'),\n",
    "                           action_distribution.histogram_bins_prob[1].alias('action_hist_1'),\n",
    "                           action_distribution.histogram_bins_prob[2].alias('action_hist_2'),\n",
    "                           action_distribution.histogram_bins_prob[3].alias('action_hist_3'),\n",
    "                           action_distribution.histogram_bins_prob[4].alias('action_hist_4'),\n",
    "                           action_distribution.histogram_bins_prob[5].alias('action_hist_5'),\n",
    "                           action_distribution.histogram_bins_prob[6].alias('action_hist_6'),\n",
    "                           action_distribution.histogram_bins_prob[7].alias('action_hist_7'),\n",
    "                           action_distribution.histogram_bins_prob[8].alias('action_hist_8'),\n",
    "                           action_distribution.histogram_bins_prob[9].alias('action_hist_9'),\n",
    "                           \"action_ts_avg\",\"action_ts_std\"\n",
    "                          )\n",
    "\n",
    "data_action = song_cnt.join(song_entropy,on='userId') \\\n",
    "              .join(artist_entropy,on='userId') \\\n",
    "              .join(song_percent,on='userId') \\\n",
    "              .join(action_distribution,on='userId') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c9919-fbcf-438a-8d83-92a05316464e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e6d4448-b258-4cd1-9c1b-ae53da6fce31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:53:16.442585Z",
     "iopub.status.busy": "2024-03-02T15:53:16.442410Z",
     "iopub.status.idle": "2024-03-02T15:53:16.717361Z",
     "shell.execute_reply": "2024-03-02T15:53:16.716760Z",
     "shell.execute_reply.started": "2024-03-02T15:53:16.442562Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ee0ad8f5e14fe0a742cd4308b9088d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inter_page = spark.sql('''\n",
    "    SELECT UserId,\n",
    "           SUM(IF(page=\"Submit Downgrade\",1,0)) as cnt_page_submitDowngrade,\n",
    "           SUM(IF(page=\"Thumbs Down\",1,0)) as cnt_page_ThumbsDown,\n",
    "           SUM(IF(page=\"Home\",1,0)) as cnt_page_Home,\n",
    "           SUM(IF(page=\"Downgrade\",1,0)) as cnt_page_Downgrade,\n",
    "           SUM(IF(page=\"Roll Advert\",1,0)) as cnt_page_RollAdvert,\n",
    "           SUM(IF(page=\"Logout\",1,0)) as cnt_page_Logout,\n",
    "           SUM(IF(page=\"Save Settings\",1,0)) as cnt_page_SaveSettings,\n",
    "           SUM(IF(page=\"About\",1,0)) as cnt_page_About,\n",
    "           SUM(IF(page=\"Settings\",1,0)) as cnt_page_Settings,\n",
    "           SUM(IF(page=\"Add to Playlist\",1,0)) as cnt_page_AddtoPlaylist,\n",
    "           SUM(IF(page=\"NextSong\",1,0)) as cnt_page_NextSong,\n",
    "           SUM(IF(page=\"Help\",1,0)) as cnt_page_Help,\n",
    "           SUM(IF(page=\"Upgrade\",1,0)) as cnt_page_upgrade,\n",
    "           SUM(IF(page=\"Error\",1,0)) as cnt_page_Error,\n",
    "           SUM(IF(page=\"Submit Upgrade\",1,0)) as cnt_page_SubmitUpgrade\n",
    "    FROM user_log_small_table \n",
    "    GROUP BY userId\n",
    "    \n",
    "''')\n",
    "\n",
    "inter_session = spark.sql('''\n",
    "    WITH CTE AS(\n",
    "        SELECT userId,\n",
    "               sessionId,\n",
    "               (MAX(ts)-MIN(ts))/(1000*3600*24) as session_duration_day\n",
    "        FROM user_log_small_table \n",
    "        GROUP BY userId,sessionId\n",
    "    )\n",
    "    \n",
    "    SELECT userId,\n",
    "           COUNT(DISTINCT sessionId) session_cnt,\n",
    "           AVG(session_duration_day) as avg_session_duration_day\n",
    "    FROM CTE\n",
    "    GROUP BY userId\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d7a3404-7298-4aaf-8d43-1cf3ae5d64c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:53:16.719097Z",
     "iopub.status.busy": "2024-03-02T15:53:16.718832Z",
     "iopub.status.idle": "2024-03-02T15:53:18.998071Z",
     "shell.execute_reply": "2024-03-02T15:53:18.996311Z",
     "shell.execute_reply.started": "2024-03-02T15:53:16.719059Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394dab04d8a6422c9f160519e2e2863d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_inter = inter_page.join(inter_session, on='userId')\n",
    "data_final = data_user.join(data_action, on='userId').join(data_inter,\n",
    "                                                           on='userId')\n",
    "\n",
    "data_final = data_final.withColumn(\"cnt_page_ThumbsDown\",data_final.cnt_page_ThumbsDown/data_final.registration_duration_day) \\\n",
    "                       .withColumn(\"cnt_page_Home\",data_final.cnt_page_Home/data_final.registration_duration_day) \\\n",
    "                       .withColumn(\"cnt_page_Downgrade\",data_final.cnt_page_Downgrade/data_final.registration_duration_day) \\\n",
    "                       .withColumn(\"cnt_page_RollAdvert\",data_final.cnt_page_RollAdvert/data_final.registration_duration_day) \\\n",
    "                       .withColumn(\"cnt_page_Logout\",data_final.cnt_page_Logout/data_final.registration_duration_day) \\\n",
    "                       .withColumn(\"cnt_page_SaveSettings\",data_final.cnt_page_SaveSettings/data_final.registration_duration_day) \\\n",
    "                       .withColumn(\"cnt_page_About\",data_final.cnt_page_About/data_final.registration_duration_day) \\\n",
    "                       .withColumn(\"cnt_page_Settings\",data_final.cnt_page_Settings/data_final.registration_duration_day) \\\n",
    "                       .withColumn(\"cnt_page_AddtoPlaylist\",data_final.cnt_page_AddtoPlaylist/data_final.registration_duration_day) \\\n",
    "                       .withColumn(\"cnt_page_NextSong\",data_final.cnt_page_NextSong/data_final.registration_duration_day) \\\n",
    "                       .withColumn(\"cnt_page_Help\",data_final.cnt_page_Help/data_final.registration_duration_day) \\\n",
    "                       .withColumn(\"cnt_page_upgrade\",data_final.cnt_page_upgrade/data_final.registration_duration_day) \\\n",
    "                       .withColumn(\"cnt_page_Error\",data_final.cnt_page_Error/data_final.registration_duration_day) \\\n",
    "                       .withColumn(\"cnt_page_SubmitUpgrade\",data_final.cnt_page_SubmitUpgrade/data_final.registration_duration_day)\n",
    "\n",
    "data_final = data_final.drop(\"userId\", \"location\", \"gender\",\n",
    "                             \"level_free_avg_duration_day\",\n",
    "                             \"level_paid_avg_duration_day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b8b41c-3a7e-43c2-a7b7-86f20d4f2181",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:53:19.009653Z",
     "iopub.status.busy": "2024-03-02T15:53:19.009478Z",
     "iopub.status.idle": "2024-03-02T15:53:19.060289Z",
     "shell.execute_reply": "2024-03-02T15:53:19.059765Z",
     "shell.execute_reply.started": "2024-03-02T15:53:19.009629Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36cbe98caf54a8ebc62f360c31ebfb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from pyspark.sql.functions import isnan, when, count, col,round,asc,desc\n",
    "#nrow = data_final.count()\n",
    "#data_final.select([round(100*count(when(col(c).isNull(), c))/nrow,2).alias(c) for c in data_final.columns]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fc139ca-4646-4792-b6e9-a29dcb2e2341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:54:12.623291Z",
     "iopub.status.busy": "2024-03-02T15:54:12.623057Z",
     "iopub.status.idle": "2024-03-02T15:55:04.191603Z",
     "shell.execute_reply": "2024-03-02T15:55:04.190904Z",
     "shell.execute_reply.started": "2024-03-02T15:54:12.623255Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c78cad20c9a49218f5d7c1a1af834e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly correlated features include:\n",
      "['registration_duration_day', 'duration_beforeFirstAction']\n",
      "['level_paid_to_free', 'cnt_page_submitDowngrade']\n",
      "['level_free_avg_duration_ratio', 'level_paid_avg_duration_ratio', 'level_free_duration_ratio', 'level_paid_duration_ratio']\n",
      "['song_cnt', 'distinct_song_cnt', 'session_cnt']\n",
      "['song_entropy', 'artist_entropy']\n",
      "['cnt_page_ThumbsDown', 'cnt_page_Home', 'cnt_page_Downgrade', 'cnt_page_Logout', 'cnt_page_AddtoPlaylist', 'cnt_page_NextSong', 'cnt_page_Help']\n",
      "\n",
      "Features to keep:\n",
      "duration_beforeFirstAction\n",
      "level_paid_to_free\n",
      "level_free_duration_ratio\n",
      "session_cnt\n",
      "artist_entropy\n",
      "cnt_page_ThumbsDown"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "vec_col = 'corr_features'   \n",
    "# assemble all vector columns into one vector column\n",
    "assembler = VectorAssembler(inputCols=data_final.columns, outputCol=vec_col)\n",
    "corr_df = assembler.transform(data_final).select(vec_col)\n",
    "\n",
    "# compute the correlation between 'churn' and every feature and the correlation between each pair of features\n",
    "corr_mat = Correlation.corr(corr_df, vec_col)\n",
    "# convert the corrlation matrix to a pandas dataframe with column names\n",
    "corr_values = corr_mat.collect()[0][0].values\n",
    "corr_mat_pd = pd.DataFrame(corr_values.reshape(-1, len(data_final.columns)), \\\n",
    "                           index=data_final.columns, columns=data_final.columns)\n",
    "\n",
    "from scipy.sparse import csr_matrix \n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "# construct an adjacency matrix where high correlation (> 0.85) is labeled as 1, otherwise 0\n",
    "is_high_corr = np.abs(corr_mat_pd.values) > 0.85\n",
    "adj_mat = csr_matrix(is_high_corr.astype(int) - np.eye(len(data_final.columns)))\n",
    "\n",
    "# find groups of highly correlated features by finding the connected components in the adjacency matrix\n",
    "_, corr_labels = connected_components(csgraph=adj_mat, directed=False)\n",
    "unique, unique_counts = np.unique(corr_labels, return_counts=True)\n",
    "# get groups with size > 1\n",
    "high_corr_labels = unique[unique_counts > 1]\n",
    "\n",
    "# if there is at least one group of highly correlated features\n",
    "if len(high_corr_labels) > 0:\n",
    "    # map the label indices of highly correlated features to their column names\n",
    "    print('Highly correlated features include:')\n",
    "    high_corr_col_dict = {}\n",
    "    for high_corr_label in high_corr_labels:\n",
    "        high_corr_col_dict[high_corr_label] = [col_name for corr_label, col_name in zip(corr_labels, data_final.columns) \n",
    "                                               if corr_label == high_corr_label]\n",
    "        print(high_corr_col_dict[high_corr_label])\n",
    "        \n",
    "    print('\\nFeatures to keep:')\n",
    "    cols_to_drop = []\n",
    "    for col_name_list in high_corr_col_dict.values(): \n",
    "        # keep the feature that has the highest correlation with 'Churn'\n",
    "        col_to_keep = corr_mat_pd.loc[col_name_list,'churn'].idxmax()\n",
    "        print(col_to_keep)\n",
    "        # remove the other features to avoid multicollinearity \n",
    "        col_name_list.remove(col_to_keep)\n",
    "        corr_mat_pd.drop(index=col_name_list, columns=col_name_list, inplace=True)\n",
    "        cols_to_drop.extend(col_name_list)\n",
    "        \n",
    "data_final = data_final.drop(*cols_to_drop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae5cfabd-8270-40c9-85a1-58d22205427b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:55:04.192686Z",
     "iopub.status.busy": "2024-03-02T15:55:04.192518Z",
     "iopub.status.idle": "2024-03-02T15:55:31.682641Z",
     "shell.execute_reply": "2024-03-02T15:55:31.681697Z",
     "shell.execute_reply.started": "2024-03-02T15:55:04.192663Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5521ea3a8fba45d082eb1b4a0f0e28a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split the data into train, validation and test sets using stratified sampling based on 'churn'\n",
    "train = data_final.sampleBy('churn', fractions={0: 0.8, 1: 0.8}, seed=42)\n",
    "test = data_final.subtract(train)\n",
    "\n",
    "\n",
    "# assign class weight\n",
    "y_collect = train.select('churn').groupBy('churn').count().collect()\n",
    "bin_counts = {y['churn']: y['count'] for y in y_collect}\n",
    "total = np.sum(e for e in bin_counts.values())\n",
    "n_labels = len(bin_counts)\n",
    "weights = {bin_: total/(n_labels*count) for bin_, count in bin_counts.items()}\n",
    "train = train.withColumn('weight', when(col('churn')==1.0, weights[1]).otherwise(weights[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b9a90-0ba8-4c24-bfe8-61cd59b9daa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26db97f1-2eed-4f8d-8d27-1cc5e09d4452",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:55:31.684294Z",
     "iopub.status.busy": "2024-03-02T15:55:31.684046Z",
     "iopub.status.idle": "2024-03-02T15:55:31.984712Z",
     "shell.execute_reply": "2024-03-02T15:55:31.983810Z",
     "shell.execute_reply.started": "2024-03-02T15:55:31.684258Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ac05f370d64d44962356252a6d307a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=[c for c in train.columns if c not in ['churn','weight']], outputCol=\"NumFeatures\")\n",
    "#scaler = StandardScaler(inputCol=\"NumFeatures\", outputCol=\"ScaledNumFeatures\", withMean=True,withStd=True)\n",
    "scaler = MinMaxScaler(inputCol=\"NumFeatures\", outputCol=\"ScaledNumFeatures\")\n",
    "\n",
    "# logistic regression\n",
    "lr = LogisticRegression(featuresCol='ScaledNumFeatures', weightCol='weight',labelCol='churn')\n",
    "pipeline_lr = Pipeline(stages=[assembler, scaler, lr])\n",
    "paramGrid_lr = ParamGridBuilder() \\\n",
    "            .addGrid(lr.regParam, [0.01, 0.1,0.5],) \\\n",
    "            .addGrid(lr.maxIter, [10, 20,40]) \\\n",
    "             .build()\n",
    "crossval_lr = CrossValidator(estimator = pipeline_lr,\n",
    "                             estimatorParamMaps = paramGrid_lr,\n",
    "                             evaluator = BinaryClassificationEvaluator(labelCol='churn', weightCol='weight',metricName='areaUnderPR'),\n",
    "                             parallelism=4,\n",
    "                             numFolds=4)\n",
    "\n",
    "# random forest\n",
    "rf = RandomForestClassifier(featuresCol='ScaledNumFeatures', weightCol='weight',labelCol='churn', seed=42,)\n",
    "pipeline_rf = Pipeline(stages=[assembler, scaler, rf])\n",
    "paramGrid_rf = ParamGridBuilder() \\\n",
    "              .addGrid(rf.numTrees, [50, 100, 150]) \\\n",
    "              .addGrid(rf.maxDepth, [10, 20]) \\\n",
    "              .build()\n",
    "crossval_rf = CrossValidator(estimator = pipeline_rf,\n",
    "                             estimatorParamMaps = paramGrid_rf,\n",
    "                             evaluator = BinaryClassificationEvaluator(labelCol='churn', weightCol='weight',metricName='areaUnderPR'),\n",
    "                             parallelism=4,\n",
    "                             numFolds=4)\n",
    "\n",
    "# SVM \n",
    "svm = LinearSVC( featuresCol=\"ScaledNumFeatures\",weightCol='weight',labelCol=\"churn\")\n",
    "pipeline_svm = Pipeline(stages=[assembler, scaler, svm])\n",
    "paramGrid_svm = ParamGridBuilder() \\\n",
    "    .addGrid(svm.maxIter, [10, 100, 1000]) \\\n",
    "    .addGrid(svm.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "crossval_svm = CrossValidator(estimator=pipeline_svm,\n",
    "                              estimatorParamMaps=paramGrid_svm,\n",
    "                              evaluator=BinaryClassificationEvaluator(labelCol='churn', weightCol='weight',                                                                     \n",
    "                                                                      metricName='areaUnderPR'),\n",
    "                                                                      parallelism=4,\n",
    "                                                                      numFolds=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8483ec58-d71a-4ce1-b46b-d4e519e3ff20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:55:31.986249Z",
     "iopub.status.busy": "2024-03-02T15:55:31.985996Z",
     "iopub.status.idle": "2024-03-02T15:55:32.043492Z",
     "shell.execute_reply": "2024-03-02T15:55:32.042712Z",
     "shell.execute_reply.started": "2024-03-02T15:55:31.986212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40538786e1c7462bb79306886d419db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def print_metrics(pred, label='churn'):\n",
    "    \"\"\"Print evaluation metrics on a test set\n",
    "    \n",
    "    Args:\n",
    "    pred: (spark dataframe) a test set \n",
    "    \n",
    "    Returns:\n",
    "    summary: (pandas dataframe) a summary of evaluation metrics\n",
    "    \"\"\"\n",
    "    eval_metrics = {}\n",
    "\n",
    "    # compute area under PR curve\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=label)\n",
    "    auc_pr = evaluator.evaluate(pred, {evaluator.metricName:'areaUnderPR'})\n",
    "    auc_roc = evaluator.evaluate(pred, {evaluator.metricName: 'areaUnderROC'})\n",
    "    # compute precision, recall and f1 score\n",
    "    predictionAndLabels = pred.select('prediction', label)\n",
    "    # both 'prediction' and label in predictionAndLabels need to be cast to float type and \n",
    "    # map to tuple before calling 'MulticlassMetrics'\n",
    "    metrics = MulticlassMetrics(predictionAndLabels.rdd.map(lambda x: tuple(map(float, x))))\n",
    "\n",
    "    # get overall statistics\n",
    "    eval_metrics['overall'] = [metrics.weightedPrecision, metrics.weightedRecall, \\\n",
    "                               metrics.weightedFMeasure(), auc_pr,auc_roc]\n",
    "                               \n",
    "    # get statistics by class\n",
    "    classes = [0.0, 1.0]\n",
    "    for cls in classes:\n",
    "        eval_metrics['class ' + str(int(cls))] = [metrics.precision(cls), metrics.recall(cls), \\\n",
    "                                                  metrics.fMeasure(cls), '','']\n",
    "\n",
    "    # convert to a pandas dataframe for display\n",
    "    summary = pd.DataFrame.from_dict(eval_metrics, orient='index', \\\n",
    "                                     columns=['precision', 'recall', 'f1 score', 'AUC-PR','AUC-ROC'])   \n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4fc1cf9-2500-4173-90af-b8660c170f01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:55:32.045680Z",
     "iopub.status.busy": "2024-03-02T15:55:32.044790Z",
     "iopub.status.idle": "2024-03-02T15:55:32.119463Z",
     "shell.execute_reply": "2024-03-02T15:55:32.118805Z",
     "shell.execute_reply.started": "2024-03-02T15:55:32.045639Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e2cbcc630e45bb82042441f6d710da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def print_metrics(pred, label='churn'):\n",
    "    \"\"\"Print evaluation metrics on a test set\n",
    "    \n",
    "    Args:\n",
    "    pred: (spark dataframe) a test set \n",
    "    \n",
    "    Returns:\n",
    "    summary: (pandas dataframe) a summary of evaluation metrics\n",
    "    \"\"\"\n",
    "    eval_metrics = {}\n",
    "\n",
    "    # compute area under PR curve\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=label)\n",
    "    auc_pr = evaluator.evaluate(pred, {evaluator.metricName:'areaUnderPR'})\n",
    "    # compute precision, recall and f1 score\n",
    "    predictionAndLabels = pred.select('prediction', label)\n",
    "    # both 'prediction' and label in predictionAndLabels need to be cast to float type and \n",
    "    # map to tuple before calling 'MulticlassMetrics'\n",
    "    metrics = MulticlassMetrics(predictionAndLabels.rdd.map(lambda x: tuple(map(float, x))))\n",
    "\n",
    "    # get overall statistics\n",
    "    eval_metrics['overall'] = [metrics.weightedPrecision, metrics.weightedRecall, \\\n",
    "                               metrics.weightedFMeasure(), auc_pr]\n",
    "                               \n",
    "    # get statistics by class\n",
    "    classes = [0.0, 1.0]\n",
    "    for cls in classes:\n",
    "        eval_metrics['class ' + str(int(cls))] = [metrics.precision(cls), metrics.recall(cls), \\\n",
    "                                                  metrics.fMeasure(cls), '']\n",
    "\n",
    "    # convert to a pandas dataframe for display\n",
    "    summary = pd.DataFrame.from_dict(eval_metrics, orient='index', \\\n",
    "                                     columns=['precision', 'recall', 'f1 score', 'AUC-PR'])   \n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e2a5944-ff12-4a37-85ba-1e1edee2eb84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:55:32.121636Z",
     "iopub.status.busy": "2024-03-02T15:55:32.121401Z",
     "iopub.status.idle": "2024-03-02T16:08:43.885569Z",
     "shell.execute_reply": "2024-03-02T16:08:43.884862Z",
     "shell.execute_reply.started": "2024-03-02T15:55:32.121599Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0535e3685f04302b92e9a32627666eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/notebook-env/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/mnt/notebook-env/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/notebook-env/lib/python3.9/site-packages/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "  File \"/mnt/notebook-env/lib/python3.9/site-packages/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "KeyError: 'jobGroup'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         precision    recall  f1 score   AUC-PR\n",
      "overall   0.863429  0.857143  0.859502  0.72207\n",
      "class 0   0.920000  0.884615  0.901961         \n",
      "class 1   0.700000  0.777778  0.736842         \n",
      "/mnt/yarn/usercache/livy/appcache/application_1709394439555_0001/container_1709394439555_0001_01_000001/pyspark.zip/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead."
     ]
    }
   ],
   "source": [
    "cv_lr = crossval_lr.fit(train)\n",
    "test_prediction = cv_lr.transform(test)\n",
    "print_metrics(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed51c13-fcec-4162-8733-15b6f01cd29f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7192d-4c51-4e5d-b8b8-a493f1c9b2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f167d356-574b-4e99-8b8f-7a59042d8aff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T16:08:43.886565Z",
     "iopub.status.busy": "2024-03-02T16:08:43.886405Z",
     "iopub.status.idle": "2024-03-02T16:21:17.459983Z",
     "shell.execute_reply": "2024-03-02T16:21:17.459306Z",
     "shell.execute_reply.started": "2024-03-02T16:08:43.886544Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c65e4e7b22a44d1b75ff184f85dd9e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-14:\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/notebook-env/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/mnt/notebook-env/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/notebook-env/lib/python3.9/site-packages/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "  File \"/mnt/notebook-env/lib/python3.9/site-packages/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "KeyError: 'jobGroup'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         precision    recall  f1 score    AUC-PR\n",
      "overall   0.972487  0.971429  0.970858  0.966232\n",
      "class 0   0.962963  1.000000  0.981132          \n",
      "class 1   1.000000  0.888889  0.941176          \n",
      "/mnt/yarn/usercache/livy/appcache/application_1709394439555_0001/container_1709394439555_0001_01_000001/pyspark.zip/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead."
     ]
    }
   ],
   "source": [
    "cv_rf = crossval_rf.fit(train)\n",
    "test_prediction = cv_rf.transform(test)\n",
    "print_metrics(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe8c8cdf-32b0-42f0-8943-8e91f3c35b17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T16:21:17.461091Z",
     "iopub.status.busy": "2024-03-02T16:21:17.460924Z",
     "iopub.status.idle": "2024-03-02T17:35:07.547775Z",
     "shell.execute_reply": "2024-03-02T17:35:07.547173Z",
     "shell.execute_reply.started": "2024-03-02T16:21:17.461068Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea8eec5285b47e7b4619632d31526d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/notebook-env/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/mnt/notebook-env/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/notebook-env/lib/python3.9/site-packages/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "  File \"/mnt/notebook-env/lib/python3.9/site-packages/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "KeyError: 'jobGroup'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         precision    recall  f1 score    AUC-PR\n",
      "overall   0.863429  0.857143  0.859502  0.733718\n",
      "class 0   0.920000  0.884615  0.901961          \n",
      "class 1   0.700000  0.777778  0.736842          \n",
      "/mnt/yarn/usercache/livy/appcache/application_1709394439555_0001/container_1709394439555_0001_01_000001/pyspark.zip/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead."
     ]
    }
   ],
   "source": [
    "cv_svm = crossval_svm.fit(train)\n",
    "test_prediction = cv_svm.transform(test)\n",
    "print_metrics(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56109472-7447-4fff-92c1-75fde32bbbb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
